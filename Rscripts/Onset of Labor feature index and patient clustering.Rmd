---
title: "Onset of Labor model training"
output: html_notebook
---

Here we are building a model similar to the one of the OOL study but on the data of the drug assay study. The code is based on the one used for the OOL study. 

## Paths

**Copy Paste Here the path to your Drug-Assay-Study folder**

```{r}
my_path<-"/Users/jonasamar/Desktop/Drug-Assay-Study"
```

```{r}
setwd(paste0(my_path,"/Rscripts"))
OOL_path=paste0(my_path,"/Onset of Labor data")
drug_assay_path=paste0(my_path,"/Drug assay data")
out_path=paste0(OOL_path,"/Prediction model")
plot_path=paste0(my_path,"/Plots/Onset of Labor prediction model")
```

Creating directories for the outputs

```{r}
if(!file.exists(out_path)){dir.create(out_path, recursive = TRUE)}
if(!file.exists(plot_path)){dir.create(plot_path, recursive = TRUE)}
```

## Libraries

```{r}
library(doSNOW)
library(parallel)
library(matrixStats)
library(zoo)
library(ggVennDiagram)
library(tidyverse)
library(readxl)
library(dplyr)
library(tibble)
library(stats)
require(Metrics)
library(missMDA)
library(FactoMineR)
set.seed(2018)
```

## Importing and preprocessing data

Preprocessing function which:
1 - Remove features with proportion of NA above a certain threshold
2 - Replace remaining NA values with the median of the feature
3 - Remove features with standard deviation below a certain threshold

```{r}
preprocessing <- function(dataset, NA_threshold, std_threshold){
  prepro_data <- dataset
  cols <- colnames(dataset)
  # Removing columns with proportion of missing values above NA_threshold
  rm_cols <- c()
  for (col_name in cols[!cols %in% "ID"]){
    na_count <- sum(is.na(prepro_data[[col_name]]))
    if (na_count/length(prepro_data[[col_name]]) > NA_threshold){
      rm_cols[length(rm_cols)+1] <- col_name
    }
  }
  prepro_data <- prepro_data[,!colnames(prepro_data) %in% rm_cols]
  cols <- colnames(prepro_data)
  # Imputing missing values with median of the colum
  for (col_name in cols[!cols %in% "ID"]){
    median <- median(prepro_data[[col_name]], na.rm = TRUE)
    prepro_data[[col_name]][is.na(prepro_data[[col_name]])] <- median
  }
  # Removing columns with standard deviation below std_threshold
  keep_cols <- which(colSds(as.matrix(prepro_data[,-which(names(prepro_data) == "ID")]))>std_threshold)
  prepro_data <- prepro_data[,names(keep_cols)]
  return(prepro_data)
}
```

Importing and preprocessing the data for DOS between DOS_inf and DOS_sup

```{r}
# When running the script to predict TTL for simulated data
predicting.simulated.data = FALSE
# DOS limits of the training data
DOS_inf=-120
DOS_sup=1
# all.simulated.data
if (predicting.simulated.data){
  load(paste0(OOL_path, "/Simulated Data/immunome_noEGA_OOL_with_drugs.rda"))
}else{
  all.simulated.data=NULL
}
# Outcomes
Yh <- read_csv(paste0(OOL_path, "/Preprocessed Data/outcome_OOL.csv"), show_col_types = FALSE)
# CYTOF data
CYTOF <- read_csv(paste0(OOL_path, "/Preprocessed Data/immunome_EGA_pen_OOL.csv"), show_col_types = FALSE)
# Reorordering and filtering CYTOF data
data <- left_join(Yh, CYTOF, by="ID") %>% filter((DOS <= DOS_sup)&(DOS>=DOS_inf))
preterm_rows <- which(grepl(paste(c("P17_", "P8_", "P3_", "P5_", "P27_"), collapse = "|"), data$ID))
Yh <- data$DOS
EGA <- data$EGA
CYTOF <- dplyr::select(data, -DOS)
CYTOF <- dplyr::select(data, -EGA) # Removing EGA
# Preprocessing
CYTOF <- preprocessing(CYTOF, 0.2, 0.)
# Patients Id
Id <- as.factor(str_extract(data[["ID"]], "(?<=P)\\d+"))
```

## Model

Helper function for predictive modeling and LOOCV
  Input arguments:
    X: data matrix with patients, and omics measurments on rows and columns, respectively.
    Y: response vector.
    foldid: patients index that is a unique id for all subjects corresponding to the same patient.
    i: the patient id that should be left out during the training process
    parm: parameters of the Lasso 
  Output args,
    ret: a list of predictions on the training and test data with the coefficients of Lasso 

The complete analysis was performed using a similar strategy but using parallel processing to optimize lambda, followed by a stack generalization layer as described in the article.

```{r}
xxx<-function(X, Y, foldid, i, parm, predict.simulated.data=FALSE)
{
  suppressMessages(library(randomForest, quietly = TRUE))
  suppressMessages(library(glmnet, quietly = TRUE))
  set.seed(2018+123*i)

  iInd=which(foldid==unique(foldid)[i])
  if(length(iInd)<2){
    iInd=c(iInd, iInd)
  }
  if(parm$scale=='ALL'){
    X=scale(X)
  }
  if(parm$scale=='Patient')
  {
    for(ap in seq(length(unique(foldid))))
    {
      sclidx= which(foldid==unique(foldid)[ap])
      if(length(sclidx)>1)
        X[sclidx]=scale(X[sclidx], scale=F)
    }
    X=scale(X) #creates NA values for 10 features
    rm_columns <- which(colSums(is.na(X)) > 0) # Getting the columns that are removed
    original.X=X # Keeping a copy of X without any columns removed
    X=X[, complete.cases(t(X))] #we remove those features
  }
  # Calculate Spearman's rank correlation and p-value
  results <- vector("list", ncol(X))
  for (j in seq_len(ncol(X))) {
    res <- cor.test(X[, j], Y, method = "spearman", use = "pairwise.complete.obs", exact = FALSE)
    results[[j]] <- data.frame(colname = colnames(X)[j], pvalue = res$p.value)
  }

  # Combine results into a data frame
  pval_df <- bind_rows(results) %>%
                mutate(pvalue = as.numeric(pvalue))

  # Removing features with pvalue under a pval_threshold
  fiter.out.features <- pval_df[pval_df$pvalue >= parm$pval_threshold, "colname"]
  X <- X[,!(colnames(X) %in% fiter.out.features)]
  rm_columns <- c(setNames(match(fiter.out.features, colnames(original.X)), fiter.out.features), rm_columns)

  if (ncol(X) < 2){
    X=cbind(X,rep(1, nrow(X)))
  }

  XX=X[-iInd,]
  YY=Y[-iInd]
  XT=X[iInd,]
  fld=as.numeric(foldid[-iInd])

  # Transforming the values of fld so that they range from 1 to 52 (avoiding one warning from glmet)
  fld[fld>=i]=fld[fld>=i]-1

  ret = list()
  
  # Removed columns
  ret$rm_cols <- rm_columns
  
  # LASSO 1SE
  cvglm = cv.glmnet(XX, YY,  standardize=F, alpha=1, foldid = fld)
  ret$p1 = predict(cvglm, XT, s='lambda.1se')
  ret$ptrain1 = predict(cvglm, XX, s='lambda.1se')
  ret$coef = coef(cvglm, s='lambda.1se')[-1]
  ret$model = cvglm
  
  # Elastic Net
  EN_cv = cv.glmnet(x = XX, y = YY, foldid = fld, alpha = parm$a)
  ret$EN_p1 = predict(EN_cv, XT, s=EN_cv$lambda.min)
  ret$EN_ptrain1 = predict(EN_cv, XX, s=EN_cv$lambda.min)
  ret$EN_coef = coef(EN_cv, s=EN_cv$lambda.min)[-1]

  # Ridge Regression
  ridge_cv = cv.glmnet(x = XX, y = YY, foldid = fld, alpha = 0)
  ret$ridge_p1 = predict(ridge_cv, XT, s=ridge_cv$lambda.min)
  ret$ridge_ptrain1 = predict(ridge_cv, XX, s=ridge_cv$lambda.min)
  ret$ridge_coef = coef(ridge_cv, s=ridge_cv$lambda.min)[-1]
  
  # Adaptive LASSO
  alasso1_cv = cv.glmnet(x = XX, y = YY, foldid = fld, alpha = 1,
                        penalty.factor = 1 / abs(ret$ridge_coef),
                        keep = TRUE)
  ret$alasso_p1 = predict(alasso1_cv, XT, s=alasso1_cv$lambda.min)
  ret$alasso_ptrain1 = predict(alasso1_cv, XX, s=alasso1_cv$lambda.min)
  ret$alasso_coef = coef(alasso1_cv, s=alasso1_cv$lambda.min)[-1]
  
  # Random Forest
  rd_forest_cv = randomForest(x = XX, y = YY, importance=TRUE)
  ret$rd_forest_p1 = predict(rd_forest_cv, XT)
  ret$rd_forest_ptrain1 = predict(rd_forest_cv, XX)
  ret$rd_forest_importance = rd_forest_cv$importance
  
  if (predict.simulated.data){
   # Predictions of Adaptive LASSO on simulated data
    for (target_drug in unique(all.simulated.data$drug)){
      # Applying CYTOF preprocessing to the simulated data
      prepro_data <- all.simulated.data %>%
        # Filtering the samples of the patients in the test set
        filter(drug==target_drug,
               grepl(paste("P",levels(foldid)[i],"_",sep=""),ID)) %>%
        # Removing the columns which were removed in CYTOF
        dplyr::select(colnames(CYTOF), -names(rm_columns)) %>%
        # Imputing the missing values with the medians of the corresponding CYTOF columns
        mutate(across(everything(), ~ ifelse(is.na(.), median(CYTOF[[cur_column()]], na.rm = TRUE), .))) %>%
        # Converting to a matrix
        column_to_rownames(var = "ID") %>%
        as.matrix()
  
      # Predictions
      if (parm$predict.model == "Random.Forest"){
        ret[[target_drug]] <- predict(rd_forest_cv, prepro_data)
      }
      if (parm$predict.model == "ElasticNet"){
        ret[[target_drug]] <- predict(EN_cv, prepro_data, s=EN_cv$lambda.min)
      }
      if (parm$predict.model == "Lasso.1se"){
        ret[[target_drug]] <- predict(cvglm, prepro_data, s='lambda.1se')
      }
    }
  }

  return(ret)
}
```

Prediction of the model

```{r}
# Function which takes the outcome of xxx and aggregate the predictions across all the models
aggregation.of.predictions <- function(prdC){
  list_name_pred.key <- list("Lasso.1se"="p1", 
                             "Ridge.Regression"="ridge_p1",
                             "ElasticNet"="EN_p1",
                             "Adaptive.Lasso"="alasso_p1",
                             "Random.Forest"="rd_forest_p1")
  list_preds <- list()
  for (model.name in names(list_name_pred.key)){
    pred.key=list_name_pred.key[[model.name]]
    ccc=vector()
    for(i in seq(npt))
    {
      iInd=which(Id==unique(Id)[i])
      if(length(iInd)>1)
      {
        ccc[iInd] = prdC[[i]][[pred.key]]
      }
      else
      {
        ccc[iInd] = prdC[[i]][[pred.key]][1]
      }
    }
    list_preds[[model.name]]=ccc
  }
  return(list_preds)
}
```

## Optimization the p-value threshold

Here we optimize the pvalue_threshold to maximize the spearmanr score of our final model

```{r}
######################### WARNING THIS CELL TAKES 8 HOURS TO RUN ######################################

# Range of thresholds tested
pval_thresholds=seq(1e-10, 0.7, length.out=101)

# xxx parameters
parm=list()
parm$scale='Patient'
parm$a=0.5
npt=length(unique(Id))

# Initializing the list of the spearamnr scores
listr<-c()

# Filling listr
for (pval_threshold in pval_thresholds){
  parm$pval_threshold=pval_threshold
  print(pval_threshold)
  # Opening backend-configuration
  cl <- makeSOCKcluster(detectCores())
  registerDoSNOW(cl)
  # Getting the predictions
  prdC=foreach(i=seq(npt),.packages = c("dplyr", "tibble")) %dopar% xxx(data.matrix(CYTOF), Yh, Id, i, parm, FALSE)
  # Closing backend configuration
  stopCluster(cl)
  # Getting the list of the best spearmanr score among the 5 models
  list_preds <- aggregation.of.predictions(prdC)
  # Extracting the maximum spearmanr score
  spearmr <- c()
  for (model.name in names(list_preds)){
    y_pred <- list_preds[[model.name]]
    spearmr <- c(spearmr, cor(Yh, y_pred, method = "spearman"))
  }
  # Adding the best spearmanr score to the list
  listr <- rbind(listr, c(pval_threshold, length(prdC[[1]]$rm_cols), spearmr, max(spearmr)))
}
# Renaming the columns
colnames(listr) <- c("pval_threshold", "n_rm_cols", names(list_preds), "best_r_score")
# Saving the dataframe
save(listr, file=(paste0(out_path, "/Pvalue threshold optimization.rda")))
```

Plotting the best pvalue threshold with their corresponding best spearmanr

```{r}
# listr
load(paste0(out_path, "/Pvalue threshold optimization.rda"))
#load(paste0(out_path, "/Pvalue threshold optimization with EGA.rda"))

# Reshaping listr for plotting
listr <- as.data.frame(listr) %>% 
  pivot_longer(cols = c("Lasso.1se","Ridge.Regression","ElasticNet","Adaptive.Lasso","Random.Forest"),
               names_to = "model", values_to = "speamanr")

# Find the p-value threshold with the highest Spearman's correlation score
best_threshold <- listr$pval_threshold[which.max(listr$speamanr)]
best_lm_threshold <- listr[listr$model!="Random.Forest",]$pval_threshold[which.max(listr[listr$model!="Random.Forest",]$speamanr)]

# Plot
p <- ggplot(listr, aes(x = pval_threshold, y = speamanr, color = model)) +
  geom_line() +
  geom_segment(x = best_threshold, xend = best_threshold, y = 0., yend=max(listr$best_r_score)+0.01, linetype = "dashed") +
  geom_segment(x = best_lm_threshold, xend = best_lm_threshold, y = 0., yend=max(listr$best_r_score)+0.01, linetype = "dashed") +
  labs(x = "p-value Threshold", y = "Spearman's correlation") +
  ggtitle("Spearman's Correlation Scores by Model") +
  theme_minimal() +
  annotate("text", x = best_threshold + 0.04, y = max(listr$best_r_score)+0.01, label = best_threshold, color = "black", size = 3, vjust = -1) +
  annotate("text", x = best_lm_threshold + 0.04, y = max(listr$best_r_score)+0.01, label = best_lm_threshold, color = "black", size = 3, vjust = -1)

# Saving plot
pdf(paste0(plot_path, "/Optimization of pvalue filtering.pdf"), height=5, width=7)
p
dev.off()
```

## Model fitting

```{r}
# xxx parameter
parm=list()
parm$scale='Patient'
parm$a=0.5
npt=length(unique(Id))
parm$pval_threshold=1

# without EGA
parm$pval_threshold=0.021
parm$predict.model="Lasso.1se"
# with EGA
# parm$pval_threshold=1e-14
# parm$predict.model="Lasso.1se"
# parm$predict.model="ElasticNet"

# Opening backend-configuration
cl <- makeSOCKcluster(detectCores())
registerDoSNOW(cl)
# Getting models and predictions
prdC=foreach(i=seq(npt),.packages = c("dplyr", "tibble")) %dopar% xxx(data.matrix(CYTOF), Yh, Id, i, parm, predicting.simulated.data)
list_preds<-aggregation.of.predictions(prdC)
# Closing backend configuration
stopCluster(cl)
```
 
## Evaluation of results

Performances of the models

```{r}
# X axis
x_axis="DOS" # EGA or DOS

# Plot function
plot.model.results <- function(y_pred, y_true, title, x_name="DOS"){
  myPv = cor.test(y_true, y_pred, method = 'spearman', exact = FALSE)$p.value
  myerr = sqrt(mean((y_true-y_pred)^2))
  myr2 = 1 - sum((y_true - y_pred)^2) / sum((y_true - mean(y_true))^2)
  mycorr = cor(y_true, y_pred, method = "spearman")
  
  if (x_name=="DOS"){dash_coefs=c(0.,1.)}
  if (x_name=="EGA"){dash_coefs=c(-259, 6.5)}

  data <- data.frame(x = y_true, y = y_pred)
  p <- ggplot(data, aes(x = x, y = y)) + 
        geom_point(na.rm=TRUE) +
        geom_abline(intercept = dash_coefs[1], slope = dash_coefs[2], linetype = "dashed") +
        labs(x = x_name, y = "Prediction") +
        scale_x_continuous(limits = c(-100, 0), expand=c(0,0)) +
        scale_y_continuous(limits = c(-100, 0), expand=c(0,0)) +
        ggtitle(paste("Model : ", title,
                      "\nSpearmanr : ", round(mycorr, digits = 5), 
                      "\nRMSE : ",round(myerr, digits = 5), 
                      "\np-value : ",round(myPv, digits = 30), 
                      "\nR^2 : ",round(myr2, digits = 5), sep=""))
  print(p)
}

# Plotting the results of the models
for (model.name in names(list_preds)){
  y_pred <- list_preds[[model.name]]
  if (x_axis=="DOS"){x=Yh}
  if (x_axis=="EGA"){x=EGA}
  pdf(file=paste0(plot_path, "/", model.name, " outcomes.pdf"), height=6, width=10)
  plot.model.results(y_pred[preterm_rows], x[preterm_rows], paste0(model.name, " on preterm"), x_axis)
  plot.model.results(y_pred[-preterm_rows], x[-preterm_rows], paste0(model.name, " on term"), x_axis)
  plot.model.results(y_pred, x, paste0(model.name, " on all"), x_axis)
  dev.off()
}
```

## Model Index

Creation of a vector of the columns that are removed when building the model.

```{r}
rm_cols <- c()
for (iter in seq(prdC)){
  rm_cols <- c(rm_cols, prdC[[iter]]$rm_cols)
}
rm_cols <- unique(rm_cols)
```

Creation the feature index with adaptive lasso.

```{r}
feature.index <- data.frame(feature = character(),
                            pval = double(),
                            mean_abs_coef = double(),
                            model_index = double(),
                            stringsAsFactors = FALSE)
cols <- colnames(CYTOF[,-c(1, rm_cols)])

for (i in seq(1, length(cols))){
  # Name of the feature
  feature <- cols[i]
  # P value based on a spearman test
  pval <- cor.test(Yh, CYTOF[[feature]], method = 'spearman', exact = FALSE)$p.value
  # Collecting all the absolute values of the coefficients of the Lasso.1se
  coefs <- c()
  for (iter in seq(prdC)){
    coefs[iter] <- abs(prdC[[iter]]$coef[i])
  }
  # Mean of the coefficients
  mean_abs_coef <- mean(coefs)
  # Adding the feature to feature.index
  feature.index <- rbind(feature.index,
                         data.frame(feature=feature,
                                    pval=pval,
                                    mean_abs_coef=mean_abs_coef,
                                    model_index=-abs(mean_abs_coef)*log10(pval),
                                    stringsAsFactors = FALSE))
}
feature.index <- feature.index %>% arrange(desc(model_index))
feature.index
```

Saving feature.index

```{r}
save(feature.index, file=paste0(out_path, "/feature_index.rda"))
```

## Overlap with OOL paper

```{r}
# List of features
x <- list(OOL_paper = c("CD56loCD16posNK_STAT1_IFNa",
                        #"Granulocytes",
                        "CD4Tnaive_MAPKAPK2_IFNa",
                        "ncMCs_CREB_GMCSF",
                        "CD8Tem_MAPKAPK2_unstim",
                        "pDCs_STAT1_IFNa",
                        "Bcells_MAPKAPK2_LPS",
                        "CD4Tem_MAPKAPK2_unstim",
                        "CD8Tem_MAPKAPK2_IFNa",
                        #"Bcells",
                        "CD4Tem_NFkB_IL246",
                        "CD4Tcm_IkB_unstim",
                        #"mDCs_STAT6_IFNa",
                        "pDCs_STAT6_IFNa",
                        #"mDCs_MAPKAPK2_unstim", 
                        "pDCs_MAPKAPK2_unstim"), 
          Top_model_index = feature.index %>%
                                dplyr::filter(model_index > 0.) %>%
                                dplyr::select(feature) %>%
                                unlist() %>%
                                as.character())

ggVennDiagram(x, label_alpha = 0)
```

## Patients clustering

Here we want to cluster the patients of the OOL data the 4 patients from the drug assay study.

```{r}
# Population_reagent_stim of interest
# population_reagent_stim_interest <- subset(feature.index, model_index > 0, select=feature) %>% 
#                                         unlist()
# Population_reagent_unstim of interest
population_reagent_unstim_interest <- subset(feature.index, model_index > 0, select=feature) %>%
                                        dplyr::mutate(feature=paste0(str_extract(feature, ".*(?=_[^_]*$)"),"_unstim")) %>%
                                        distinct(feature) %>%
                                        unlist()
# Setting the time to labor of the four patients from the drug assay study
centroids.TTL <- c(-84,-84,-84,-84)
# Getting the centroids (4 patients)
drug_assay_unstim_0 <- subset(read.csv(paste(drug_assay_path, "/Preprocessed Data/preprocessed_pen.csv", sep = "/")),
                                             dose==-1,
                                             select= -c(X, dose)) %>%
                                       dplyr::mutate(feature.name=paste(population, reagent, stimulation, sep="_")) %>%
                                       dplyr::filter(feature.name %in% population_reagent_unstim_interest)

# drug_assay_stim_0 <- subset(read.csv(paste(drug_assay_path, "/Preprocessed Data/preprocessed_pen.csv", sep = "/")),
#                                              (dose %in% c(-1,0) & !(stimulation=="unstim" & dose==0) ), #
#                                              select= -c(X,dose)) %>%
#                                      dplyr::mutate(feature.name=paste(population, reagent, stimulation, sep="_")) %>%
#                                      dplyr::filter(feature.name %in% population_reagent_stim_interest)

centroids <- drug_assay_unstim_0 %>%
                    group_by(feature.name, ID) %>%
                    dplyr::summarise(feature=median(feature)) %>%
                    pivot_wider(names_from = ID, values_from = feature) %>%
                    column_to_rownames("feature.name")

# Joining the TTL prediction and the Id of the patients to the CYTOF data
OOL_data <- read_csv(paste0(OOL_path, "/Preprocessed Data/immunome_EGA_pen_OOL.csv"), show_col_types = FALSE)
OOL_unstim <- OOL_data[, (grepl("unstim", colnames(OOL_data)) & (colnames(OOL_data) %in% rownames(centroids)))] %>%
                cbind(data.frame("TTL"=list_preds[[parm$predict.model]], "ID"=Id))
# OOL_stim <- OOL_data[, colnames(OOL_data) %in% rownames(centroids)] %>%
#                 cbind(data.frame("TTL"=list_preds[[parm$predict.model]], "ID"=Id))

# Normalization of the data
for (target_feature in rownames(centroids)){
  # Mean and std from drug assay data
  mu_DA <- subset(drug_assay_unstim_0, feature.name==target_feature, select=feature) %>% unlist() %>% mean(na.rm=T)
  sd_DA <- subset(drug_assay_unstim_0, feature.name==target_feature, select=feature) %>% unlist() %>% sd(na.rm=T)
  # Mean and std from OOL data
  mu_OOL <- subset(OOL_data, select=target_feature) %>% unlist() %>% mean(na.rm=T)
  sd_OOL <- subset(OOL_data, select=target_feature) %>% unlist() %>% sd(na.rm=T)
  # Rescaling the features
  centroids[target_feature,] <- (centroids[target_feature,] - mu_DA)/sd_DA
  OOL_unstim[,target_feature] <- (OOL_unstim[,target_feature] - mu_OOL)/sd_OOL
}
# Adding the TTL predictions to the centroids
centroids["TTL",] <- centroids.TTL

# Clusters initialization
clusters <- list(c(), c(), c(), c())
data_points <- c()
# Filling the cluster patient per patient
for (id in unique(Id)){
  # Getting all the samples from the same patient
  samples <- subset(OOL_unstim, ID==id, select=-ID) %>% remove_rownames() %>% t()
  colnames(samples) <- seq(colnames(samples))

  # Calculating the euclidian distances between the patient and the centroids
  list_dist <- list()
  for (centroid_id in seq(centroids.TTL)){
    centroid_data <- centroids %>% dplyr::select(centroid_id) %>% setNames("centroid")
    dist_euclid <- cbind(centroid_data, samples) %>%
                      # Calculating (sample - centroid)^2
                      dplyr::mutate(across(where(is.numeric), ~ (. - centroid)^2)) %>%
                      # Selecting sample with closest TTL to the centroid TTL
                      dplyr::select(-centroid) %>%
                      t() %>%
                      as.data.frame() %>%
                      dplyr::filter(TTL==min(TTL)) %>%
                      # Calculating euclidian distance with centroid
                      dplyr::select(-TTL)%>%
                      unlist() %>%
                      sum(na.rm=T) %>%
                      sqrt()
    list_dist[[centroid_id]] <- dist_euclid
  }
  # Assigning the id to one of the four clusters
  closest_centroid <- which.min(unlist(list_dist))
  clusters[[closest_centroid]] <- c(clusters[[closest_centroid]], id)
  # Getting the sample used for the clustering
  sample <- samples %>% t() %>% as.data.frame() %>% 
              dplyr::mutate(dist.TTL=abs(TTL-centroids.TTL[closest_centroid]),
                            cluster=closest_centroid) %>% 
              subset(dist.TTL==min(dist.TTL), select=-c(TTL, dist.TTL)) 
  data_points <- rbind(data_points, sample)
}

# Creating new directory
if (!file.exists(paste0(OOL_path, "/Patients clustering"))){dir.create(paste0(OOL_path, "/Patients clustering"), recursive = TRUE)}

# Saving the clusters
save(clusters, file=paste0(OOL_path, "/Patients clustering/Cluster 53 OOL patients around 4 drug assay patients.rda"))

# Adding the centroids to the data_points used for the clustering
colnames(centroids) = c("centroid1","centroid2","centroid3","centroid4")
data_points <- rbind(data_points,
                     centroids[!(rownames(centroids) %in% c("TTL")),] %>% t() %>% cbind(matrix(c(1, 2, 3, 4), ncol=1, nrow=4, dimnames=list(NULL, "cluster"))))
```
Plot of the clusters

```{r}
# Filter out variables with variance not equal to zero
filtered_data <- data_points %>% select_if(~sd(., na.rm = TRUE) != 0)

# Separating cluster number from actual coordinates
patient_clusters  <- data_points$cluster %>% as.character()
data <- dplyr::select(filtered_data, -cluster)

# Imputing missing values and determining the optimal number of principal components to represent our data
nb <- estim_ncpPCA(data,method.cv = "Kfold", verbose = FALSE)
res.comp <- imputePCA(data, ncp = nb$ncp)

# Using PCA to project the points on a 2D surface
imp <- cbind.data.frame(res.comp$completeObs,patient_clusters)
res.pca <- PCA(imp, quanti.sup = 1, quali.sup = 13, ncp = nb$ncp, graph=FALSE)

# Dataframes for plot
projected_coord <- cbind(res.pca$ind$coord[,c("Dim.1", "Dim.2")], patient_clusters) 
df_coord <- as.data.frame(projected_coord) %>% 
              rownames_to_column("ID") %>%
              dplyr::filter(!grepl("centroid", ID)) %>% 
              dplyr::mutate(Dim.1=as.double(Dim.1), Dim.2=as.double(Dim.2))
df_centroid <- as.data.frame(projected_coord) %>% 
              rownames_to_column("ID") %>% 
              dplyr::filter(grepl("centroid", ID))%>% 
              dplyr::mutate(Dim.1=as.double(Dim.1), Dim.2=as.double(Dim.2))

# Plot with all the points
p1 <- ggplot() +
  geom_point(data=df_centroid, aes(x=Dim.1, y=Dim.2, color=patient_clusters, shape="Drug assay centroids"), size=3) +
  geom_point(data=df_coord, aes(x=Dim.1, y=Dim.2, color=patient_clusters, shape="OOL patients")) +
  scale_shape_manual(name = "Patients", values = c("OOL patients" = 21, "Drug assay centroids" = 8)) +
  guides(shape = guide_legend(title = "Patients", order=2), color=guide_legend(title = "Clusters", order=1)) +
  scale_x_continuous(limits=c(-5, 5), expand=c(0., 0.)) +
  scale_y_continuous(limits=c(-3, 3), expand=c(0., 0.)) +
  ggtitle("Projection of patient clustering on 2 first dimensions of PCA analysis") +
  theme_minimal()

# Plot with only the centroids
p2 <- ggplot() +
  geom_point(data=df_centroid, aes(x=Dim.1, y=Dim.2, color=patient_clusters, shape="Drug assay centroids"), size=3) +
  scale_shape_manual(name = "Patients", values = c("OOL patients" = 21, "Drug assay centroids" = 8)) +
  guides(shape = guide_legend(title = "Patients", order=2), color=guide_legend(title = "Clusters", order=1)) +
  scale_x_continuous(limits=c(-5, 5), expand=c(0., 0.)) +
  scale_y_continuous(limits=c(-3, 3), expand=c(0., 0.)) +
  ggtitle("Projection of centroids on 2 first dimensions of PCA analysis") +
  theme_minimal()

# Creating new directory
if (!file.exists(paste0(my_path, "/Plots/Clustering"))){dir.create(paste0(my_path, "/Plots/Clustering"), recursive = TRUE)}

# Saving the clustering plots
pdf(paste0(my_path, "/Plots/Clustering/Projection of patient clustering with PCA dim1&2.pdf"), height=5, width=7)
p1
p2
dev.off()
```

## <<<<<<<<<<<<<<<<<<<<<<<<<<<Simulated drug effect visualization>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

Matrix of all the simulated predictions

```{r}
simulated.data.predictions <- c()
ID_order <- all.simulated.data[all.simulated.data$drug=="Benzylpenicillin",]$ID 
for (drug in unique(all.simulated.data$drug)){
  drug.predictions <- c()
  for (i in seq(prdC)){
    drug.predictions <- rbind(drug.predictions, as.matrix(prdC[[i]][[drug]]))
  }
  col.names <- c(colnames(simulated.data.predictions), drug)
  ID_order <- intersect(ID_order, rownames(drug.predictions))
  simulated.data.predictions <- cbind(simulated.data.predictions, 
                                      drug.predictions[ID_order,])
  colnames(simulated.data.predictions) <- col.names
}
head(simulated.data.predictions)
```

Filtering the samples corresponding to DOS between DOS.inf and DOS.sup days

```{r}
# Adaptive.Lasso (cst) GOOD
#DOS.inf <- -94
#DOS.sup <- -74
# ElasticNet (cst) GOOD-
#DOS.inf <- -84
#DOS.sup <- -70
# Random.Forest (cst) GOOD-
#DOS.inf <- -84
#DOS.sup <- -70
# Adaptive.Lasso (prop) GOOD+
#DOS.inf <- -81
#DOS.sup <- -73
# ElasticNet (prop) GOOD+
DOS.inf <- -83
DOS.sup <- -70
DOS.inf <- -120
DOS.sup <- -60
# Random.Forest (prop) GOOD-
#DOS.inf <- -82
#DOS.sup <- -70
filter.ID <- subset(read_csv(paste0(OOL_path, "/outcome_OOL.csv"), show_col_types = FALSE),
                    DOS >= DOS.inf & DOS <= DOS.sup,
                    select = ID)$ID
```

Plotting the distributions of the difference between estimated TTL and estimated TTL with simulated drug effect.

```{r}
# Getting the row numbers of the predictions corresponding to simulated.data
outcomes <- read_csv(paste0(OOL_path, "/outcome_OOL.csv"), show_col_types = FALSE)
outcomes <- outcomes[match(CYTOF$ID, outcomes$ID), ]
row_numbers <- which(outcomes$ID %in% ID_order)

# TTL predictions
TTL.predictions <- matrix(list_preds[[parm$predict.model]][row_numbers], 
                          nrow = dim(simulated.data.predictions)[1], 
                          ncol = dim(simulated.data.predictions)[2], 
                          dimnames = list(ID_order, NULL))[ID_order,]

# Difference between the TTL predictions and the TTL predictions with simulated drug effect
drug.effect.on.predictions <- - (simulated.data.predictions - TTL.predictions)

# Looking at the distributions of the drug simulated effect in term of days on the TTL prediction
data <- as.data.frame(drug.effect.on.predictions[filter.ID,])

# Reshape data into long format
data_long <- gather(data, key = "drug", value = "value")

# Plot distributions
ggplot(data_long, aes(x = value, fill = drug)) +
  geom_density(alpha = 0.5) +
  xlab("Delay (in days)") +
  geom_vline(xintercept = 0, linetype = "dashed") +
  theme_minimal()
```
