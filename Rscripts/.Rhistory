#
#   # Removing features with pvalue under a pval_threshold
#   fiter.out.features <- pval_df[pval_df$pvalue >= parm$pval_threshold, "colname"]
#   X <- X[,!(colnames(X) %in% fiter.out.features)]
#   rm_columns <- c(setNames(match(fiter.out.features, colnames(original.X)), fiter.out.features), rm_columns)
if (ncol(X) < 2){
X=cbind(X,rep(1, nrow(X)))
}
XX=X[-iInd,]
YY=Y[-iInd]
XT=X[iInd,]
fld=as.numeric(foldid[-iInd])
# Transforming the values of fld so that they range from 1 to 52 (avoiding one warning from glmet)
fld[fld>=i]=fld[fld>=i]-1
ret = list()
# Removed columns
ret$rm_cols <- rm_columns
# LASSO 1SE
cvglm = cv.glmnet(XX, YY,  standardize=F, alpha=1, foldid = fld)
ret$p1 = predict(cvglm, XT, s='lambda.1se')
ret$ptrain1 = predict(cvglm, XX, s='lambda.1se')
ret$coef = coef(cvglm, s='lambda.1se')[-1]
ret$model = cvglm
# Elastic Net
EN_cv = cv.glmnet(x = XX, y = YY, foldid = fld, alpha = parm$a)
ret$EN_p1 = predict(EN_cv, XT, s=EN_cv$lambda.min)
ret$EN_ptrain1 = predict(EN_cv, XX, s=EN_cv$lambda.min)
ret$EN_coef = coef(EN_cv, s=EN_cv$lambda.min)[-1]
# Ridge Regression
ridge_cv = cv.glmnet(x = XX, y = YY, foldid = fld, alpha = 0)
ret$ridge_p1 = predict(ridge_cv, XT, s=ridge_cv$lambda.min)
ret$ridge_ptrain1 = predict(ridge_cv, XX, s=ridge_cv$lambda.min)
ret$ridge_coef = coef(ridge_cv, s=ridge_cv$lambda.min)[-1]
# Adaptive LASSO
alasso1_cv = cv.glmnet(x = XX, y = YY, foldid = fld, alpha = 1,
penalty.factor = 1 / abs(ret$ridge_coef),
keep = TRUE)
ret$alasso_p1 = predict(alasso1_cv, XT, s=alasso1_cv$lambda.min)
ret$alasso_ptrain1 = predict(alasso1_cv, XX, s=alasso1_cv$lambda.min)
ret$alasso_coef = coef(alasso1_cv, s=alasso1_cv$lambda.min)[-1]
# Random Forest
rd_forest_cv = randomForest(x = XX, y = YY, importance=TRUE)
ret$rd_forest_p1 = predict(rd_forest_cv, XT)
ret$rd_forest_ptrain1 = predict(rd_forest_cv, XX)
ret$rd_forest_importance = rd_forest_cv$importance
if (predict.simulated.data){
# Predictions of Adaptive LASSO on simulated data
for (target_drug in unique(all.simulated.data$drug)){
# Applying CYTOF preprocessing to the simulated data
prepro_data <- all.simulated.data %>%
# Filtering the samples of the patients in the test set
filter(drug==target_drug,
grepl(paste("P",levels(foldid)[i],"_",sep=""),ID)) %>%
# Removing the columns which were removed in CYTOF
dplyr::select(colnames(CYTOF), -names(rm_columns)) %>%
# Imputing the missing values with the medians of the corresponding CYTOF columns
mutate(across(everything(), ~ ifelse(is.na(.), median(CYTOF[[cur_column()]], na.rm = TRUE), .))) %>%
# Converting to a matrix
column_to_rownames(var = "ID") %>%
as.matrix()
# Predictions
if (parm$predict.model == "Random.Forest"){
ret[[target_drug]] <- predict(rd_forest_cv, prepro_data)
}
if (parm$predict.model == "ElasticNet"){
ret[[target_drug]] <- predict(EN_cv, prepro_data, s=EN_cv$lambda.min)
}
if (parm$predict.model == "Lasso.1se"){
ret[[target_drug]] <- predict(cvglm, prepro_data, s='lambda.1se')
}
}
}
return(ret)
}
parm=list()
parm$scale='Patient'
parm$a=0.5
npt=length(unique(Id))
parm$pval_threshold=1
xxx(data.matrix(CYTOF[,"EGA"]), Yh, Id, 1, parm, FALSE)
data.matrix(CYTOF[,"EGA"])
xxx<-function(X, Y, foldid, i, parm, predict.simulated.data=FALSE)
{
suppressMessages(library(randomForest, quietly = TRUE))
suppressMessages(library(glmnet, quietly = TRUE))
set.seed(2018+123*i)
iInd=which(foldid==unique(foldid)[i])
if(length(iInd)<2){
iInd=c(iInd, iInd)
}
if(parm$scale=='ALL'){
X=scale(X)
}
if(parm$scale=='Patient')
{
for(ap in seq(length(unique(foldid))))
{
sclidx= which(foldid==unique(foldid)[ap])
if(length(sclidx)>1)
X[sclidx]=scale(X[sclidx], scale=F)
}
# X=scale(X) #creates NA values for 10 features
# rm_columns <- which(colSums(is.na(X)) > 0) # Getting the columns that are removed
# original.X=X # Keeping a copy of X without any columns removed
# X=X[, complete.cases(t(X))] #we remove those features
}
# Calculate Spearman's rank correlation and p-value
# results <- vector("list", ncol(X))
# for (j in seq_len(ncol(X))) {
#   res <- cor.test(X[, j], Y, method = "spearman", use = "pairwise.complete.obs", exact = FALSE)
#   results[[j]] <- data.frame(colname = colnames(X)[j], pvalue = res$p.value)
# }
#
#   # Combine results into a data frame
#   pval_df <- bind_rows(results) %>%
#                 mutate(pvalue = as.numeric(pvalue))
#
#   # Removing features with pvalue under a pval_threshold
#   fiter.out.features <- pval_df[pval_df$pvalue >= parm$pval_threshold, "colname"]
#   X <- X[,!(colnames(X) %in% fiter.out.features)]
#   rm_columns <- c(setNames(match(fiter.out.features, colnames(original.X)), fiter.out.features), rm_columns)
if (ncol(X) < 2){
X=cbind(X,rep(1, nrow(X)))
}
XX=X[-iInd,]
YY=Y[-iInd]
XT=X[iInd,]
fld=as.numeric(foldid[-iInd])
# Transforming the values of fld so that they range from 1 to 52 (avoiding one warning from glmet)
fld[fld>=i]=fld[fld>=i]-1
ret = list()
# Removed columns
ret$rm_cols <- rm_columns
# LASSO 1SE
cvglm = cv.glmnet(XX, YY,  standardize=F, alpha=1, foldid = fld)
ret$p1 = predict(cvglm, XT, s='lambda.1se')
ret$ptrain1 = predict(cvglm, XX, s='lambda.1se')
ret$coef = coef(cvglm, s='lambda.1se')[-1]
ret$model = cvglm
print("lasso")
# Elastic Net
EN_cv = cv.glmnet(x = XX, y = YY, foldid = fld, alpha = parm$a)
ret$EN_p1 = predict(EN_cv, XT, s=EN_cv$lambda.min)
ret$EN_ptrain1 = predict(EN_cv, XX, s=EN_cv$lambda.min)
ret$EN_coef = coef(EN_cv, s=EN_cv$lambda.min)[-1]
print("EN")
# Ridge Regression
ridge_cv = cv.glmnet(x = XX, y = YY, foldid = fld, alpha = 0)
ret$ridge_p1 = predict(ridge_cv, XT, s=ridge_cv$lambda.min)
ret$ridge_ptrain1 = predict(ridge_cv, XX, s=ridge_cv$lambda.min)
ret$ridge_coef = coef(ridge_cv, s=ridge_cv$lambda.min)[-1]
print("Ridge")
# Adaptive LASSO
alasso1_cv = cv.glmnet(x = XX, y = YY, foldid = fld, alpha = 1,
penalty.factor = 1 / abs(ret$ridge_coef),
keep = TRUE)
ret$alasso_p1 = predict(alasso1_cv, XT, s=alasso1_cv$lambda.min)
ret$alasso_ptrain1 = predict(alasso1_cv, XX, s=alasso1_cv$lambda.min)
ret$alasso_coef = coef(alasso1_cv, s=alasso1_cv$lambda.min)[-1]
print("ADA")
# Random Forest
rd_forest_cv = randomForest(x = XX, y = YY, importance=TRUE)
ret$rd_forest_p1 = predict(rd_forest_cv, XT)
ret$rd_forest_ptrain1 = predict(rd_forest_cv, XX)
ret$rd_forest_importance = rd_forest_cv$importance
print("Rd")
if (predict.simulated.data){
# Predictions of Adaptive LASSO on simulated data
for (target_drug in unique(all.simulated.data$drug)){
# Applying CYTOF preprocessing to the simulated data
prepro_data <- all.simulated.data %>%
# Filtering the samples of the patients in the test set
filter(drug==target_drug,
grepl(paste("P",levels(foldid)[i],"_",sep=""),ID)) %>%
# Removing the columns which were removed in CYTOF
dplyr::select(colnames(CYTOF), -names(rm_columns)) %>%
# Imputing the missing values with the medians of the corresponding CYTOF columns
mutate(across(everything(), ~ ifelse(is.na(.), median(CYTOF[[cur_column()]], na.rm = TRUE), .))) %>%
# Converting to a matrix
column_to_rownames(var = "ID") %>%
as.matrix()
# Predictions
if (parm$predict.model == "Random.Forest"){
ret[[target_drug]] <- predict(rd_forest_cv, prepro_data)
}
if (parm$predict.model == "ElasticNet"){
ret[[target_drug]] <- predict(EN_cv, prepro_data, s=EN_cv$lambda.min)
}
if (parm$predict.model == "Lasso.1se"){
ret[[target_drug]] <- predict(cvglm, prepro_data, s='lambda.1se')
}
}
}
return(ret)
}
xxx(data.matrix(CYTOF[,"EGA"]), Yh, Id, 1, parm, FALSE)
xxx(data.matrix(EGA), Yh, Id, 1, parm, FALSE)
xxx<-function(X, Y, foldid, i, parm, predict.simulated.data=FALSE)
{
suppressMessages(library(randomForest, quietly = TRUE))
suppressMessages(library(glmnet, quietly = TRUE))
set.seed(2018+123*i)
iInd=which(foldid==unique(foldid)[i])
if(length(iInd)<2){
iInd=c(iInd, iInd)
}
if(parm$scale=='ALL'){
X=scale(X)
}
if(parm$scale=='Patient')
{
for(ap in seq(length(unique(foldid))))
{
sclidx= which(foldid==unique(foldid)[ap])
if(length(sclidx)>1)
X[sclidx]=scale(X[sclidx], scale=F)
}
# X=scale(X) #creates NA values for 10 features
# rm_columns <- which(colSums(is.na(X)) > 0) # Getting the columns that are removed
# original.X=X # Keeping a copy of X without any columns removed
# X=X[, complete.cases(t(X))] #we remove those features
}
# Calculate Spearman's rank correlation and p-value
# results <- vector("list", ncol(X))
# for (j in seq_len(ncol(X))) {
#   res <- cor.test(X[, j], Y, method = "spearman", use = "pairwise.complete.obs", exact = FALSE)
#   results[[j]] <- data.frame(colname = colnames(X)[j], pvalue = res$p.value)
# }
#
#   # Combine results into a data frame
#   pval_df <- bind_rows(results) %>%
#                 mutate(pvalue = as.numeric(pvalue))
#
#   # Removing features with pvalue under a pval_threshold
#   fiter.out.features <- pval_df[pval_df$pvalue >= parm$pval_threshold, "colname"]
#   X <- X[,!(colnames(X) %in% fiter.out.features)]
#   rm_columns <- c(setNames(match(fiter.out.features, colnames(original.X)), fiter.out.features), rm_columns)
if (ncol(X) < 2){
X=cbind(X,rep(1, nrow(X)))
}
XX=X[-iInd,]
YY=Y[-iInd]
XT=X[iInd,]
fld=as.numeric(foldid[-iInd])
# Transforming the values of fld so that they range from 1 to 52 (avoiding one warning from glmet)
fld[fld>=i]=fld[fld>=i]-1
ret = list()
# Removed columns
#ret$rm_cols <- rm_columns
# LASSO 1SE
cvglm = cv.glmnet(XX, YY,  standardize=F, alpha=1, foldid = fld)
ret$p1 = predict(cvglm, XT, s='lambda.1se')
ret$ptrain1 = predict(cvglm, XX, s='lambda.1se')
ret$coef = coef(cvglm, s='lambda.1se')[-1]
ret$model = cvglm
print("lasso")
# Elastic Net
EN_cv = cv.glmnet(x = XX, y = YY, foldid = fld, alpha = parm$a)
ret$EN_p1 = predict(EN_cv, XT, s=EN_cv$lambda.min)
ret$EN_ptrain1 = predict(EN_cv, XX, s=EN_cv$lambda.min)
ret$EN_coef = coef(EN_cv, s=EN_cv$lambda.min)[-1]
print("EN")
# Ridge Regression
ridge_cv = cv.glmnet(x = XX, y = YY, foldid = fld, alpha = 0)
ret$ridge_p1 = predict(ridge_cv, XT, s=ridge_cv$lambda.min)
ret$ridge_ptrain1 = predict(ridge_cv, XX, s=ridge_cv$lambda.min)
ret$ridge_coef = coef(ridge_cv, s=ridge_cv$lambda.min)[-1]
print("Ridge")
# Adaptive LASSO
alasso1_cv = cv.glmnet(x = XX, y = YY, foldid = fld, alpha = 1,
penalty.factor = 1 / abs(ret$ridge_coef),
keep = TRUE)
ret$alasso_p1 = predict(alasso1_cv, XT, s=alasso1_cv$lambda.min)
ret$alasso_ptrain1 = predict(alasso1_cv, XX, s=alasso1_cv$lambda.min)
ret$alasso_coef = coef(alasso1_cv, s=alasso1_cv$lambda.min)[-1]
print("ADA")
# Random Forest
rd_forest_cv = randomForest(x = XX, y = YY, importance=TRUE)
ret$rd_forest_p1 = predict(rd_forest_cv, XT)
ret$rd_forest_ptrain1 = predict(rd_forest_cv, XX)
ret$rd_forest_importance = rd_forest_cv$importance
print("Rd")
if (predict.simulated.data){
# Predictions of Adaptive LASSO on simulated data
for (target_drug in unique(all.simulated.data$drug)){
# Applying CYTOF preprocessing to the simulated data
prepro_data <- all.simulated.data %>%
# Filtering the samples of the patients in the test set
filter(drug==target_drug,
grepl(paste("P",levels(foldid)[i],"_",sep=""),ID)) %>%
# Removing the columns which were removed in CYTOF
dplyr::select(colnames(CYTOF), -names(rm_columns)) %>%
# Imputing the missing values with the medians of the corresponding CYTOF columns
mutate(across(everything(), ~ ifelse(is.na(.), median(CYTOF[[cur_column()]], na.rm = TRUE), .))) %>%
# Converting to a matrix
column_to_rownames(var = "ID") %>%
as.matrix()
# Predictions
if (parm$predict.model == "Random.Forest"){
ret[[target_drug]] <- predict(rd_forest_cv, prepro_data)
}
if (parm$predict.model == "ElasticNet"){
ret[[target_drug]] <- predict(EN_cv, prepro_data, s=EN_cv$lambda.min)
}
if (parm$predict.model == "Lasso.1se"){
ret[[target_drug]] <- predict(cvglm, prepro_data, s='lambda.1se')
}
}
}
return(ret)
}
xxx(data.matrix(EGA), Yh, Id, 1, parm, FALSE)
xxx(data.matrix(CYTOF[,"EGA"]), Yh, Id, 1, parm, FALSE)
xxx<-function(X, Y, foldid, i, parm, predict.simulated.data=FALSE)
{
suppressMessages(library(randomForest, quietly = TRUE))
suppressMessages(library(glmnet, quietly = TRUE))
set.seed(2018+123*i)
iInd=which(foldid==unique(foldid)[i])
if(length(iInd)<2){
iInd=c(iInd, iInd)
}
if(parm$scale=='ALL'){
X=scale(X)
}
if(parm$scale=='Patient')
{
for(ap in seq(length(unique(foldid))))
{
sclidx= which(foldid==unique(foldid)[ap])
if(length(sclidx)>1)
X[sclidx]=scale(X[sclidx], scale=F)
}
# X=scale(X) #creates NA values for 10 features
# rm_columns <- which(colSums(is.na(X)) > 0) # Getting the columns that are removed
# original.X=X # Keeping a copy of X without any columns removed
# X=X[, complete.cases(t(X))] #we remove those features
}
# Calculate Spearman's rank correlation and p-value
# results <- vector("list", ncol(X))
# for (j in seq_len(ncol(X))) {
#   res <- cor.test(X[, j], Y, method = "spearman", use = "pairwise.complete.obs", exact = FALSE)
#   results[[j]] <- data.frame(colname = colnames(X)[j], pvalue = res$p.value)
# }
#
#   # Combine results into a data frame
#   pval_df <- bind_rows(results) %>%
#                 mutate(pvalue = as.numeric(pvalue))
#
#   # Removing features with pvalue under a pval_threshold
#   fiter.out.features <- pval_df[pval_df$pvalue >= parm$pval_threshold, "colname"]
#   X <- X[,!(colnames(X) %in% fiter.out.features)]
#   rm_columns <- c(setNames(match(fiter.out.features, colnames(original.X)), fiter.out.features), rm_columns)
if (ncol(X) < 2){
X=cbind(X,rep(1, nrow(X)))
}
XX=X[-iInd,]
YY=Y[-iInd]
XT=X[iInd,]
fld=as.numeric(foldid[-iInd])
# Transforming the values of fld so that they range from 1 to 52 (avoiding one warning from glmet)
fld[fld>=i]=fld[fld>=i]-1
ret = list()
# Removed columns
#ret$rm_cols <- rm_columns
# LASSO 1SE
cvglm = cv.glmnet(XX, YY,  standardize=F, alpha=1, foldid = fld)
ret$p1 = predict(cvglm, XT, s='lambda.1se')
ret$ptrain1 = predict(cvglm, XX, s='lambda.1se')
ret$coef = coef(cvglm, s='lambda.1se')[-1]
ret$model = cvglm
# Elastic Net
EN_cv = cv.glmnet(x = XX, y = YY, foldid = fld, alpha = parm$a)
ret$EN_p1 = predict(EN_cv, XT, s=EN_cv$lambda.min)
ret$EN_ptrain1 = predict(EN_cv, XX, s=EN_cv$lambda.min)
ret$EN_coef = coef(EN_cv, s=EN_cv$lambda.min)[-1]
# Ridge Regression
ridge_cv = cv.glmnet(x = XX, y = YY, foldid = fld, alpha = 0)
ret$ridge_p1 = predict(ridge_cv, XT, s=ridge_cv$lambda.min)
ret$ridge_ptrain1 = predict(ridge_cv, XX, s=ridge_cv$lambda.min)
ret$ridge_coef = coef(ridge_cv, s=ridge_cv$lambda.min)[-1]
# Adaptive LASSO
alasso1_cv = cv.glmnet(x = XX, y = YY, foldid = fld, alpha = 1,
penalty.factor = 1 / abs(ret$ridge_coef),
keep = TRUE)
ret$alasso_p1 = predict(alasso1_cv, XT, s=alasso1_cv$lambda.min)
ret$alasso_ptrain1 = predict(alasso1_cv, XX, s=alasso1_cv$lambda.min)
ret$alasso_coef = coef(alasso1_cv, s=alasso1_cv$lambda.min)[-1]
# Random Forest
rd_forest_cv = randomForest(x = XX, y = YY, importance=TRUE)
ret$rd_forest_p1 = predict(rd_forest_cv, XT)
ret$rd_forest_ptrain1 = predict(rd_forest_cv, XX)
ret$rd_forest_importance = rd_forest_cv$importance
if (predict.simulated.data){
# Predictions of Adaptive LASSO on simulated data
for (target_drug in unique(all.simulated.data$drug)){
# Applying CYTOF preprocessing to the simulated data
prepro_data <- all.simulated.data %>%
# Filtering the samples of the patients in the test set
filter(drug==target_drug,
grepl(paste("P",levels(foldid)[i],"_",sep=""),ID)) %>%
# Removing the columns which were removed in CYTOF
dplyr::select(colnames(CYTOF), -names(rm_columns)) %>%
# Imputing the missing values with the medians of the corresponding CYTOF columns
mutate(across(everything(), ~ ifelse(is.na(.), median(CYTOF[[cur_column()]], na.rm = TRUE), .))) %>%
# Converting to a matrix
column_to_rownames(var = "ID") %>%
as.matrix()
# Predictions
if (parm$predict.model == "Random.Forest"){
ret[[target_drug]] <- predict(rd_forest_cv, prepro_data)
}
if (parm$predict.model == "ElasticNet"){
ret[[target_drug]] <- predict(EN_cv, prepro_data, s=EN_cv$lambda.min)
}
if (parm$predict.model == "Lasso.1se"){
ret[[target_drug]] <- predict(cvglm, prepro_data, s='lambda.1se')
}
}
}
return(ret)
}
# xxx parameter
parm=list()
parm$scale='Patient'
parm$a=0.5
npt=length(unique(Id))
parm$pval_threshold=1
# Random.Forest settings
#parm$pval_threshold=0.056
#parm$predict.model="Random.Forest"
# ElasticNet settings
#parm$pval_threshold=0.5075
#parm$predict.model="ElasticNet"
# Lasso1.se settings
#parm$pval_threshold=0.5
#parm$predict.model="Lasso.1se"
# with EGA
#parm$pval_threshold=1e-14
#parm$predict.model="Lasso.1se"
# Opening backend-configuration
cl <- makeSOCKcluster(detectCores())
registerDoSNOW(cl)
# Getting models and predictions
prdC=foreach(i=seq(npt),.packages = c("dplyr", "tibble")) %dopar% xxx(data.matrix(EGA), Yh, Id, i, parm, FALSE) #data.matrix(CYTOF)
list_preds<-aggregation.of.predictions(prdC)
# Closing backend configuration
stopCluster(cl)
# X axis
x_axis="EGA" # EGA or DOS
require(Metrics)
# Plot function
plot.model.results <- function(y_pred, y_true, title, x_name="DOS"){
myPv = cor.test(y_true, y_pred, method = 'spearman', exact = FALSE)$p.value
myerr = sqrt(mean((y_true-y_pred)^2))
myr2 = 1 - sum((y_true - y_pred)^2) / sum((y_true - mean(y_true))^2)
mycorr = cor(y_true, y_pred, method = "spearman")
if (x_name=="DOS"){dash_coefs=c(0.,1.)}
if (x_name=="EGA"){dash_coefs=c(-259, 6.5)}
data <- data.frame(x = y_true, y = y_pred)
p <- ggplot(data, aes(x = x, y = y)) +
geom_point() +
geom_abline(intercept = dash_coefs[1], slope = dash_coefs[2], linetype = "dashed") +
labs(x = x_name, y = "Prediction") +
ggtitle(paste("Model : ", title,
"\nSpearmanr : ", round(mycorr, digits = 5),
"\nRMSE : ",round(myerr, digits = 5),
"\np-value : ",round(myPv, digits = 30),
"\nR^2 : ",round(myr2, digits = 5), sep=""))
print(p)
}
# Plotting the results of the models
for (model.name in names(list_preds)){
y_pred <- list_preds[[model.name]]
if (x_axis=="DOS"){x=Yh}
if (x_axis=="EGA"){x=EGA}
plot.model.results(y_pred[preterm_rows], x[preterm_rows], paste0(model.name, " on preterm"), x_axis)
plot.model.results(y_pred[-preterm_rows], x[-preterm_rows], paste0(model.name, " on term"), x_axis)
plot.model.results(y_pred, x, paste0(model.name, " on all"), x_axis)
}
# X axis
x_axis="DOS" # EGA or DOS
require(Metrics)
# Plot function
plot.model.results <- function(y_pred, y_true, title, x_name="DOS"){
myPv = cor.test(y_true, y_pred, method = 'spearman', exact = FALSE)$p.value
myerr = sqrt(mean((y_true-y_pred)^2))
myr2 = 1 - sum((y_true - y_pred)^2) / sum((y_true - mean(y_true))^2)
mycorr = cor(y_true, y_pred, method = "spearman")
if (x_name=="DOS"){dash_coefs=c(0.,1.)}
if (x_name=="EGA"){dash_coefs=c(-259, 6.5)}
data <- data.frame(x = y_true, y = y_pred)
p <- ggplot(data, aes(x = x, y = y)) +
geom_point() +
geom_abline(intercept = dash_coefs[1], slope = dash_coefs[2], linetype = "dashed") +
labs(x = x_name, y = "Prediction") +
ggtitle(paste("Model : ", title,
"\nSpearmanr : ", round(mycorr, digits = 5),
"\nRMSE : ",round(myerr, digits = 5),
"\np-value : ",round(myPv, digits = 30),
"\nR^2 : ",round(myr2, digits = 5), sep=""))
print(p)
}
# Plotting the results of the models
for (model.name in names(list_preds)){
y_pred <- list_preds[[model.name]]
if (x_axis=="DOS"){x=Yh}
if (x_axis=="EGA"){x=EGA}
plot.model.results(y_pred[preterm_rows], x[preterm_rows], paste0(model.name, " on preterm"), x_axis)
plot.model.results(y_pred[-preterm_rows], x[-preterm_rows], paste0(model.name, " on term"), x_axis)
plot.model.results(y_pred, x, paste0(model.name, " on all"), x_axis)
}
