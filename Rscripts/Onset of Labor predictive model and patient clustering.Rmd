---
title: "Onset of Labor predictive model and patients clustering"
output: html_notebook
---

Here we are building a model similar to the one of the OOL study but on the data of the drug assay study. The code is based on the one used for the OOL study. 

## Paths

Here we set all the necessary paths to import and save data and plots.

```{r}
current_dir <- getwd()
my_path <- sub("/[^/]+$", "", current_dir)
setwd(paste0(my_path,"/Rscripts"))
OOL_path=paste0(my_path,"/Onset of Labor data")
drug_assay_path=paste0(my_path,"/Drug assay data")
out_path=paste0(OOL_path,"/Prediction model")
plot_path=paste0(my_path,"/Plots/Onset of Labor prediction model")
```

Creating directories for the outputs

```{r}
if(!file.exists(out_path)){dir.create(out_path, recursive = TRUE)}
if(!file.exists(plot_path)){dir.create(plot_path, recursive = TRUE)}
```

## Libraries

```{r}
library(doSNOW)
library(parallel)
library(matrixStats)
library(zoo)
library(ggVennDiagram)
library(tidyverse)
library(readxl)
library(dplyr)
library(tibble)
library(stats)
require(Metrics)
library(missMDA)
library(FactoMineR)
set.seed(2018)
```

## Importing and preprocessing data

Preprocessing function which:
1 - Remove features with proportion of NA above a certain threshold
2 - Replace remaining NA values with the median of the feature
3 - Remove features with standard deviation below a certain threshold

```{r}
preprocessing <- function(dataset, NA_threshold, std_threshold){
  prepro_data <- dataset
  cols <- colnames(dataset)
  # Removing columns with proportion of missing values above NA_threshold
  rm_cols <- c()
  for (col_name in cols[!cols %in% "ID"]){
    na_count <- sum(is.na(prepro_data[[col_name]]))
    if (na_count/length(prepro_data[[col_name]]) > NA_threshold){
      rm_cols[length(rm_cols)+1] <- col_name
    }
  }
  prepro_data <- prepro_data[,!colnames(prepro_data) %in% rm_cols]
  cols <- colnames(prepro_data)
  # Imputing missing values with median of the colum
  for (col_name in cols[!cols %in% "ID"]){
    median <- median(prepro_data[[col_name]], na.rm = TRUE)
    prepro_data[[col_name]][is.na(prepro_data[[col_name]])] <- median
  }
  # Removing columns with standard deviation below std_threshold
  keep_cols <- which(colSds(as.matrix(prepro_data[,-which(names(prepro_data) == "ID")]))>std_threshold)
  prepro_data <- prepro_data[,names(keep_cols)]
  return(prepro_data)
}
```

Importing and preprocessing the data for DOS between DOS_inf and DOS_sup

```{r}
# median.simulated.data
if (file.exists(paste0(OOL_path, "/Simulated Data/median simulated data.rda"))){
  load(paste0(OOL_path, "/Simulated Data/median simulated data.rda"))
}
# individual.simulated.data
if (file.exists(paste0(OOL_path, "/Simulated Data/individual simulated data.rda"))){
  load(paste0(OOL_path, "/Simulated Data/individual simulated data.rda"))
}
# Outcomes
Yh <- read_csv(paste0(OOL_path, "/Preprocessed Data/outcome_OOL.csv"), show_col_types = FALSE)
# CYTOF data
CYTOF <- read_csv(paste0(OOL_path, "/Preprocessed Data/immunome_EGA_pen_OOL.csv"), show_col_types = FALSE)
# Reorordering and filtering CYTOF data
data <- left_join(Yh, CYTOF, by="ID")
preterm_rows <- which(grepl(paste(c("P17_", "P8_", "P3_", "P5_", "P27_"), collapse = "|"), data$ID))
CYTOF <- dplyr::select(data, -DOS)
CYTOF <- dplyr::select(CYTOF, -EGA) # Removing EGA
Yh <- data$DOS
EGA <- data$EGA
# Preprocessing
CYTOF <- preprocessing(CYTOF, 0.2, 0.)
# Patients Id
Id <- as.factor(str_extract(data[["ID"]], "(?<=P)\\d+"))
```

## Model

Helper function for predictive modeling and LOOCV
  Input arguments:
    X: data matrix with patients, and omics measurments on rows and columns, respectively.
    Y: response vector.
    foldid: patients index that is a unique id for all subjects corresponding to the same patient.
    i: the patient id that should be left out during the training process
    parm: parameters of the Lasso 
  Output args,
    ret: a list of predictions on the training and test data with the coefficients of Lasso 

The complete analysis was performed using a similar strategy but using parallel processing to optimize lambda, followed by a stack generalization layer as described in the article.

```{r}
xxx<-function(X, Y, foldid, i, parm, predict.simulated.data=FALSE)
{
  suppressMessages(library(randomForest, quietly = TRUE))
  suppressMessages(library(glmnet, quietly = TRUE))
  set.seed(2018+123*i)
  
  original.X=X # Keeping a copy of X without any columns removed and before scaling

  iInd=which(foldid==unique(foldid)[i])
  if(length(iInd)<2){
    iInd=c(iInd, iInd)
  }
  if(parm$scale=='ALL'){
    X=scale(X)
  }
  if(parm$scale=='Patient')
  {
    sclidx= which(foldid==unique(foldid)[i])
    if(length(sclidx)>1){
      patient.col.means = colMeans(X[sclidx,]) # Storing means to apply same scaling to simulated data
    }else{
      patient.col.means = rep(0., dim(X)[2]) 
      names(patient.col.means) = colnames(X)
    }

    for(ap in seq(length(unique(foldid))))
    {
      sclidx= which(foldid==unique(foldid)[ap])
      if(length(sclidx)>1)
        X[sclidx]=scale(X[sclidx], scale=F)
    }
  
    X=scale(X) #creates NA values for 10 features
    rm_columns <- which(colSums(is.na(X)) > 0) # Getting the columns that are removed
    X=X[, complete.cases(t(X))] #we remove those features
  }
  # Calculate Spearman's rank correlation and p-value
  results <- vector("list", ncol(X))
  for (j in seq_len(ncol(X))) {
    res <- cor.test(X[, j], Y, method = "spearman", use = "pairwise.complete.obs", exact = FALSE)
    results[[j]] <- data.frame(colname = colnames(X)[j], pvalue = res$p.value)
  }

  # Combine results into a data frame
  pval_df <- bind_rows(results) %>%
                mutate(pvalue = as.numeric(pvalue))

  # Removing features with pvalue under a pval_threshold
  fiter.out.features <- pval_df[pval_df$pvalue >= parm$pval_threshold, "colname"]
  X <- X[,!(colnames(X) %in% fiter.out.features)]
  rm_columns <- c(setNames(match(fiter.out.features, colnames(original.X)), fiter.out.features), rm_columns)

  if (ncol(X) < 2){
    X=cbind(X,rep(1, nrow(X)))
  }

  XX=X[-iInd,]
  YY=Y[-iInd]
  XT=X[iInd,]
  fld=as.numeric(foldid[-iInd])

  # Transforming the values of fld so that they range from 1 to 52 (avoiding one warning from glmet)
  fld[fld>=i]=fld[fld>=i]-1

  ret = list()

  # Removed columns
  ret$rm_cols <- rm_columns
  
  # LASSO 1SE
  cvglm = cv.glmnet(XX, YY,  standardize=F, alpha=1, foldid = fld)
  ret$p1 = predict(cvglm, XT, s='lambda.1se')
  ret$ptrain1 = predict(cvglm, XX, s='lambda.1se')
  ret$coef = coef(cvglm, s='lambda.1se')[-1]
  ret$model = cvglm
  
  # Elastic Net
  EN_cv = cv.glmnet(x = XX, y = YY, standardize=F, foldid = fld, alpha = parm$a)
  ret$EN_p1 = predict(EN_cv, XT, s=EN_cv$lambda.min)
  ret$EN_ptrain1 = predict(EN_cv, XX, s=EN_cv$lambda.min)
  ret$EN_coef = coef(EN_cv, s=EN_cv$lambda.min)[-1]

  # Ridge Regression
  ridge_cv = cv.glmnet(x = XX, y = YY, standardize=F, foldid = fld, alpha = 0)
  ret$ridge_p1 = predict(ridge_cv, XT, s=ridge_cv$lambda.min)
  ret$ridge_ptrain1 = predict(ridge_cv, XX, s=ridge_cv$lambda.min)
  ret$ridge_coef = coef(ridge_cv, s=ridge_cv$lambda.min)[-1]
  
  # Adaptive LASSO
  alasso1_cv = cv.glmnet(x = XX, y = YY, standardize=F, foldid = fld, alpha = 1,
                        penalty.factor = 1 / abs(ret$ridge_coef),
                        keep = TRUE)
  ret$alasso_p1 = predict(alasso1_cv, XT, s=alasso1_cv$lambda.min)
  ret$alasso_ptrain1 = predict(alasso1_cv, XX, s=alasso1_cv$lambda.min)
  ret$alasso_coef = coef(alasso1_cv, s=alasso1_cv$lambda.min)[-1]
  
  # Random Forest
  rd_forest_cv = randomForest(x = XX, y = YY, standardize=F, importance=TRUE)
  ret$rd_forest_p1 = predict(rd_forest_cv, XT)
  ret$rd_forest_ptrain1 = predict(rd_forest_cv, XX)
  ret$rd_forest_importance = rd_forest_cv$importance
  
  if (predict.simulated.data){
    # Getting X before scaling but only with the features which were filtered
    unscaled.X = original.X[,!(colnames(original.X) %in% ret$rm_cols)]
    means = colMeans(unscaled.X)
    sds = colSds(unscaled.X)
    
    # Filtering the samples of the patients in the test set
    ret$median.simulated.TTL <- subset(median.simulated.data, grepl(paste0("P",unique(foldid)[i],"_"),sampleID))
    ret$individual.simulated.TTL <- subset(individual.simulated.data, grepl(paste0("P",unique(foldid)[i],"_"),sampleID))

    # Applying CYTOF preprocessing to the median simulated data
    median_prepro_data <- dplyr::select(ret$median.simulated.TTL, -c(drug, sampleID, centroid, dose)) %>%
        # Removing the columns which were removed in CYTOF
        dplyr::select(intersect(colnames(original.X),colnames(ret$median.simulated.TTL)), -names(ret$rm_cols)) %>%
        # Imputing the missing values with the medians of the corresponding CYTOF columns
        dplyr::mutate(across(everything(), ~ ifelse(is.na(.), median(CYTOF[[cur_column()]], na.rm = TRUE), .))) %>%
        # Applying the patient and gobal scaling applyed to the training data
        dplyr::mutate(across(everything(), ~ (. - patient.col.means[[cur_column()]] - means[[cur_column()]])/sds[[cur_column()]])) %>%
        # Converting to a matrix
        as.matrix()
    
    # Applying CYTOF preprocessing to the individual simulated data
    individual_prepro_data <- dplyr::select(ret$individual.simulated.TTL, -c(drug, sampleID, centroid, dose)) %>%
        # Removing the columns which were removed in CYTOF
        dplyr::select(intersect(colnames(original.X),colnames(ret$individual.simulated.TTL)), -names(ret$rm_cols)) %>%
        # Imputing the missing values with the medians of the corresponding CYTOF columns
        dplyr::mutate(across(everything(), ~ ifelse(is.na(.), median(CYTOF[[cur_column()]], na.rm = TRUE), .))) %>%
        # Applying the patient and gobal scaling applyed to the training data
        dplyr::mutate(across(everything(), ~ (. - patient.col.means[[cur_column()]] - means[[cur_column()]])/sds[[cur_column()]])) %>%
        # Converting to a matrix
        as.matrix()

    # Predictions
    if (parm$predict.model == "Random.Forest"){
      median_predictions <- predict(rd_forest_cv, median_prepro_data)
      individual_predictions <- predict(rd_forest_cv, individual_prepro_data)
    }else if(parm$predict.model == "ElasticNet"){
      median_predictions <- predict(EN_cv, median_prepro_data, s=EN_cv$lambda.min)
      individual_predictions <- predict(EN_cv, individual_prepro_data, s=EN_cv$lambda.min)
    }else if(parm$predict.model == "Lasso.1se"){
      median_predictions <- predict(cvglm, median_prepro_data, s='lambda.1se')
      individual_predictions <- predict(cvglm, individual_prepro_data, s='lambda.1se')
    }else if(parm$predict.model == "Ridge.Regression"){
      median_predictions <- predict(ridge_cv, median_prepro_data, s=ridge_cv$lambda.min)
      individual_predictions <- predict(ridge_cv, individual_prepro_data, s=ridge_cv$lambda.min)
    }else if(parm$predict.model == "Adaptive.Lasso"){
      median_predictions <- predict(alasso1_cv, median_prepro_data, s=alasso1_cv$lambda.min)
      individual_predictions <- predict(alasso1_cv, individual_prepro_data, s=alasso1_cv$lambda.min)
    }else{
      median_predictions <- rep(NA, dim(median_prepro_data)[0])
      individual_predictions <- rep(NA, dim(individual_prepro_data)[0])
    }
    
    ret$median.simulated.TTL <-cbind(dplyr::select(ret$median.simulated.TTL, c(drug, sampleID, centroid, dose)),
                                     median_predictions)
    ret$individual.simulated.TTL <-cbind(dplyr::select(ret$individual.simulated.TTL, c(drug, sampleID, centroid, dose)),
                                     individual_predictions)
  }

  return(ret)
}
```

Prediction of the model

```{r}
# Function which takes the outcome of xxx and aggregate the predictions across all the models
aggregation.of.predictions <- function(prdC){
  list_name_pred.key <- list("Lasso.1se"="p1", 
                             "Ridge.Regression"="ridge_p1",
                             "ElasticNet"="EN_p1",
                             "Adaptive.Lasso"="alasso_p1",
                             "Random.Forest"="rd_forest_p1")
  list_preds <- list()
  for (model.name in names(list_name_pred.key)){
    pred.key=list_name_pred.key[[model.name]]
    ccc=vector()
    for(i in seq(npt))
    {
      iInd=which(Id==unique(Id)[i])
      if(length(iInd)>1)
      {
        ccc[iInd] = prdC[[i]][[pred.key]]
      }
      else
      {
        ccc[iInd] = prdC[[i]][[pred.key]][1]
      }
    }
    list_preds[[model.name]]=ccc
  }
  return(list_preds)
}
```

## Optimization the p-value threshold

Here we optimize the pvalue_threshold to maximize the spearmanr score of our final model

```{r}
######################### WARNING THIS CELL TAKES 8 HOURS TO RUN ######################################

# Range of thresholds tested
pval_thresholds=seq(1e-10, 0.7, length.out=101)

# xxx parameters
parm=list()
parm$scale='Patient'
parm$a=0.5
npt=length(unique(Id))

# Initializing the list of the spearamnr scores
listr<-c()

# Filling listr
for (pval_threshold in pval_thresholds){
  parm$pval_threshold=pval_threshold
  print(pval_threshold)
  # Opening backend-configuration
  cl <- makeSOCKcluster(detectCores())
  registerDoSNOW(cl)
  # Getting the predictions
  prdC=foreach(i=seq(npt),.packages = c("dplyr", "tibble", "matrixStats"), .export = c("CYTOF")) %dopar% xxx(data.matrix(CYTOF), Yh, Id, i, parm, FALSE)
  # Closing backend configuration
  stopCluster(cl)
  # Getting the list of the best spearmanr score among the 5 models
  list_preds <- aggregation.of.predictions(prdC)
  # Extracting the maximum spearmanr score
  spearmr <- c()
  for (model.name in names(list_preds)){
    y_pred <- list_preds[[model.name]]
    spearmr <- c(spearmr, cor(Yh, y_pred, method = "spearman"))
  }
  # Adding the best spearmanr score to the list
  listr <- rbind(listr, c(pval_threshold, length(prdC[[1]]$rm_cols), spearmr, max(spearmr)))
}
# Renaming the columns
colnames(listr) <- c("pval_threshold", "n_rm_cols", names(list_preds), "best_r_score")
# Saving the dataframe
save(listr, file=(paste0(out_path, "/Pvalue threshold optimization.rda")))
```

Plotting the best pvalue threshold with their corresponding best spearmanr (you can look at the plot in the folder /Plots/Onset of labor prediction model)

```{r}
# listr
load(paste0(out_path, "/Pvalue threshold optimization.rda"))

# Reshaping listr for plotting
listr <- as.data.frame(listr) %>% 
  pivot_longer(cols = c("Lasso.1se","Ridge.Regression","ElasticNet","Adaptive.Lasso","Random.Forest"),
               names_to = "model", values_to = "speamanr")

# Find the p-value threshold with the highest Spearman's correlation score
best_forest_threshold <- listr[listr$model=="Random.Forest",]$pval_threshold[which.max(listr[listr$model=="Random.Forest",]$speamanr)]
best_lasso_threshold <- listr[listr$model=="Lasso.1se",]$pval_threshold[which.max(listr[listr$model=="Lasso.1se",]$speamanr)]
best_EN_threshold <- listr[listr$model=="ElasticNet",]$pval_threshold[which.max(listr[listr$model=="ElasticNet",]$speamanr)]
best_ridge_threshold <- listr[listr$model=="Ridge.Regression",]$pval_threshold[which.max(listr[listr$model=="Ridge.Regression",]$speamanr)]
best_alasso_threshold <- listr[listr$model=="Adaptive.Lasso",]$pval_threshold[which.max(listr[listr$model=="Adaptive.Lasso",]$speamanr)]

# Plot
p <- ggplot(listr, aes(x = pval_threshold, y = speamanr, color = model)) +
  geom_line() +
  geom_segment(x = best_forest_threshold, xend = best_forest_threshold, y = 0., yend=max(listr$best_r_score)+0.01, linetype = "dashed") +
  geom_segment(x = best_lasso_threshold, xend = best_lasso_threshold, y = 0., yend=max(listr$best_r_score)+0.01, linetype = "dashed") +
  geom_segment(x = best_ridge_threshold, xend = best_ridge_threshold, y = 0., yend=max(listr$best_r_score)+0.01, linetype = "dashed") +
  geom_segment(x = best_EN_threshold, xend = best_EN_threshold, y = 0., yend=max(listr$best_r_score)+0.01, linetype = "dashed") +
  geom_segment(x = best_alasso_threshold, xend = best_alasso_threshold, y = 0., yend=max(listr$best_r_score)+0.01, linetype = "dashed") +
  labs(x = "p-value Threshold", y = "Spearman's correlation") +
  ggtitle("Spearman's Correlation Scores by Model") +
  theme_minimal() +
  annotate("text", x = best_forest_threshold + 0.045, y = max(listr$best_r_score)+0.01, label = best_forest_threshold, size = 3, vjust = -1) +
  annotate("text", x = best_alasso_threshold + 0.05, y = max(listr$best_r_score)+0.02, label = best_alasso_threshold, size = 3, vjust = -1) +
  annotate("text", x = best_ridge_threshold + 0.04, y = max(listr$best_r_score)+0.015, label = best_ridge_threshold, size = 3, vjust = -1) +
  annotate("text", x = best_lasso_threshold , y = max(listr$best_r_score)+0.01, label = best_lasso_threshold, size = 3, vjust = -1) +
  annotate("text", x = best_EN_threshold, y = max(listr$best_r_score)+0.015, label = best_EN_threshold, size = 3, vjust = -1)
    

# Saving plot
pdf(paste0(plot_path, "/Optimization of pvalue filtering.pdf"), height=5, width=7)
p
dev.off()
```

Creating a list of the best pvalue thresholds

```{r}
best_pval_thresholds <- list("Lasso.1se"=best_lasso_threshold, 
                             "Ridge.Regression"=best_ridge_threshold,
                             "ElasticNet"=best_EN_threshold,
                             "Adaptive.Lasso"=best_alasso_threshold,
                             "Random.Forest"=best_forest_threshold)
```

## Building the best models according to pvalue thresholding optimizaton
                             
```{r}
######################### WARNING : THIS CELL TAKES 1H30 TO RUN ! #########################

# Building the best models and storing them and their predictions
for (model.name in names(best_pval_thresholds)){
  # xxx parameter
  parm=list()
  parm$scale='Patient'
  parm$a=0.5
  npt=length(unique(Id))
  parm$predict.model=model.name
  parm$pval_threshold=best_pval_thresholds[[model.name]]
  # Opening backend-configuration
  cl <- makeSOCKcluster(detectCores())
  registerDoSNOW(cl)
  # Getting models and predictions
  prdC=foreach(i=seq(npt),.packages = c("dplyr", "tibble", "matrixStats")) %dopar% xxx(data.matrix(CYTOF), Yh, Id, i, parm, TRUE)
  list_preds<-aggregation.of.predictions(prdC)
  # Closing backend configuration
  stopCluster(cl)
  # Saving the models and their predictions
  save(prdC, file=paste0(out_path, "/best ", model.name,".rda"))
  save(list_preds, file=paste0(out_path, "/best ", model.name," predictions.rda"))
}
```

## Model fitting with EGA

```{r}
# Getting CYTOF with EGA
CYTOF_EGA <- dplyr::select(data, -DOS)
# Preprocessing
CYTOF_EGA <- preprocessing(CYTOF_EGA, 0.2, 0.)

# xxx parameter
parm=list()
parm$scale='Patient'
parm$a=0.5
npt=length(unique(Id))
parm$pval_threshold=1e-14
parm$predict.model="ElasticNet"

# Opening backend-configuration
cl <- makeSOCKcluster(detectCores())
registerDoSNOW(cl)
# Getting models and predictions
prdC=foreach(i=seq(npt),.packages = c("dplyr", "tibble", "matrixStats"), .export=c("CYTOF")) %dopar% xxx(data.matrix(CYTOF_EGA), Yh, Id, i, parm, TRUE)
list_preds<-aggregation.of.predictions(prdC)
# Closing backend configuration
stopCluster(cl)
```
 
## Evaluation of results

Visualization of the performances of the models (you can look at the plot in the folder /Plots/Onset of labor prediction model).

```{r}
# X axis
x_axis="EGA" # EGA or DOS

# Plot function
plot.model.results <- function(y_pred, y_true, ega, title, x_name="DOS"){
  myPv = cor.test(y_true, y_pred, method = 'spearman', exact = FALSE)$p.value
  myerr = sqrt(mean((y_true-y_pred)^2))
  myr2 = 1 - sum((y_true - y_pred)^2) / sum((y_true - mean(y_true))^2)
  mycorr = cor(y_true, y_pred, method = "spearman")
  
  if (x_name=="DOS"){
    data <- data.frame(x = y_true, y = y_pred)
    linear.model.fit <- lm(y_pred ~ y_true, data)
    dash_coefs=c(coef(linear.model.fit)[["(Intercept)"]],coef(linear.model.fit)[["y_true"]])
    y_reg = dash_coefs[1] + dash_coefs[2]*y_true
    myr2 = 1 - sum((y_reg - y_pred)^2) / sum((y_pred - mean(y_pred))^2)
    x.limits <- c(-110, 0)
    y.limits <- c(-110, 0)
  }else if (x_name=="EGA"){
    data <- data.frame(x = ega, y = y_pred)
    linear.model.fit <- lm(y ~ x, data)
    dash_coefs=c(coef(linear.model.fit)[["(Intercept)"]],coef(linear.model.fit)[["x"]])
    y_reg = dash_coefs[1] + dash_coefs[2]*ega
    myr2 = 1 - sum((y_reg - y_pred)^2) / sum((y_pred - mean(y_pred))^2)
    x.limits <- c(22,35)
    y.limits <- c(-110, 0)
  }else{
    dash_coefs=c(0., 1.)
    data <- data.frame(x = NA, y = NA)
    myr2 = NA
    x.limits <- c(-1, 0)
    y.limits <- c(-1, 0)
  }

  p <- ggplot(data, aes(x = x, y = y)) + 
        geom_point(fill="black", shape=21, color = "black", size = 2., na.rm = TRUE) +
        geom_abline(intercept = dash_coefs[1], slope = dash_coefs[2], linetype = "dashed", color = "black") +
        labs(x = x_name, y = "Prediction") +
        scale_x_continuous(limits = x.limits, expand=c(0,0)) +
        scale_y_continuous(limits = y.limits, expand=c(0,0)) +
        ggtitle(paste("Model : ", title,
                      "\nSpearmanr : ", round(mycorr, digits = 5), 
                      "\nRMSE : ",round(myerr, digits = 5), 
                      "\np-value : ",round(myPv, digits = 30), 
                      "\nR^2 : ",round(myr2, digits = 5), sep="")) +
        theme(plot.title = element_text(size = 12),
              panel.background = element_rect(fill = NA, color = NA),
              plot.background = element_rect(fill = NA, color = NA),
              axis.title.x = element_text(size = 12),
              axis.title.y = element_text(size = 12),
              axis.text.x = element_text(size = 10),
              axis.text.y = element_text(size = 10),
              axis.line.x = element_line(),
              axis.line.y = element_line())
    
    if (x_name == "DOS"){ p <- p + coord_fixed()}
  
  print(p)
}

# Plotting the results of the best models
for (model.name in names(best_pval_thresholds)){
  load(paste0(out_path, "/best ", model.name," predictions.rda"))
  pval_threshold <- best_pval_thresholds[[model.name]]
  y_pred <- list_preds[[model.name]]
  pdf(file=paste0(plot_path, "/", model.name, " (p<",pval_threshold, ") on ",x_axis," outcomes.pdf"))
  plot.model.results(y_pred[preterm_rows], Yh[preterm_rows], EGA[preterm_rows], paste0(model.name, " on preterm"), x_axis)
  plot.model.results(y_pred[-preterm_rows], Yh[-preterm_rows], EGA[-preterm_rows], paste0(model.name, " on term"), x_axis)
  plot.model.results(y_pred, Yh, EGA, paste0(model.name, " on all"), x_axis)
  dev.off()
}
```

## Model Index

```{r}
get.feature.index <- function(model.name, on.best.feature=FALSE, CYTOF.data = CYTOF){
  #Importing selected model and its predictions
  if (on.best.feature){
    # list_preds
    load(paste0(out_path, "/", model.name," on best features predictions.rda"))
    # prdC
    load(paste0(out_path, "/", model.name," on best features.rda"))
  }else{
    # list_preds
    load(paste0(out_path, "/best ", model.name," predictions.rda"))
    # prdC
    load(paste0(out_path, "/best ", model.name,".rda"))
  }
  
  # key to fetch the model's coefs in prdC
  if (model.name == "Random.Forest"){
    coef_key <- "rd_forest_importance"
  }else if(model.name == "ElasticNet"){
    coef_key <- "EN_coef"
  }else if(model.name == "Lasso.1se"){
    coef_key <- "coef"
  }else if(model.name == "Ridge.Regression"){
    coef_key <- "ridge_coef"
  }else if(model.name == "Adaptive.Lasso"){
    coef_key <- "alasso_coef"
  }else{
    coef_key <- NULL
  }
  
  # Creation of a vector of the columns that are removed when building the model.
  rm_cols <- c()
  for (iter in seq(prdC)){
    rm_cols <- c(rm_cols, prdC[[iter]]$rm_cols)
  }
  rm_cols <- unique(rm_cols)

  # Creation of the feature index
  if (model.name=="Random.Forest"){
    feature.index <- data.frame(feature = character(),
                              pval = double(),
                              mean_abs_coef = double(),
                              model_index = double(),
                              stringsAsFactors = FALSE)
  }else{
    feature.index <- data.frame(feature = character(),
                              pval = double(),
                              mean_relative_node_purity = double(),
                              model_index = double(),
                              stringsAsFactors = FALSE)
  }
  
  if (length(rm_cols) > 0){
    cols <- colnames(CYTOF.data[,-rm_cols])
  }else{
    cols <- colnames(CYTOF.data)
  }
  
  for (i in seq(1, length(cols))){
    # Name of the feature
    feature <- cols[i]
    # P value based on a spearman test
    pval <- cor.test(Yh, CYTOF.data[[feature]], method = 'spearman', exact = FALSE)$p.value
    # Coefficient of relative node purity given by the model
    if (model.name=="Random.Forest"){
      # Collecting all the node purity of the selected features
      relative_node_purity <- c()
      for (iter in seq(prdC)){
        relative_node_purity[iter] <- prdC[[iter]][[coef_key]][feature,"IncNodePurity"]/
          max(prdC[[iter]][[coef_key]][,"IncNodePurity"])
      }
      # Mean of the relative node purity
      mean_relative_node_purity <- mean(relative_node_purity)
      # Adding the feature to feature.index
      feature.index <- rbind(feature.index,
                             data.frame(feature=feature,
                                        pval=pval,
                                        mean_relative_node_purity=mean_relative_node_purity,
                                        model_index=-mean_relative_node_purity*log10(pval),
                                        stringsAsFactors = FALSE))
    }else{
      # Collecting all the absolute values of the coefficients of the model
      coefs <- c()
      for (iter in seq(prdC)){
        coefs[iter] <- abs(prdC[[iter]][[coef_key]][i])
      }
      # Mean of the coefficients
      mean_abs_coef <- mean(coefs)
      # Adding the feature to feature.index
      feature.index <- rbind(feature.index,
                             data.frame(feature=feature,
                                        pval=pval,
                                        mean_abs_coef=mean_abs_coef,
                                        model_index=-abs(mean_abs_coef)*log10(pval),
                                        stringsAsFactors = FALSE))
    }
  }
  feature.index <- feature.index %>% arrange(desc(model_index))
}
```

Example of model index for Adaptive.Lasso

```{r}
feature.index <- get.feature.index("Adaptive.Lasso")
feature.index
```

## Models on best features

Getting the features selected by all the best models

```{r}
# Getting the intersection of all selected features by the 5 models
best.features <- unique(AllPenDoseresponse$feature)
for (model.name in names(best_pval_thresholds)){
  feature.index <- get.feature.index(model.name)
  selected.features <- feature.index[feature.index[,"model_index"]>0, "feature"]
  best.features <- intersect(best.features, selected.features)
}

# Saving the list of features selected by all models
save(feature.index, file=paste0(out_path, "/feature_index.rda"))

# Extracting the CYTOF dataset restricted to these features
restricted.CYTOF <- dplyr::select(CYTOF, best.features)
```

Building models on the best features only

```{r}
######################### WARNING : THIS CELL TAKES 1H30 TO RUN ! #########################

# Building the best models and storing them and their predictions
for (model.name in names(best_pval_thresholds)){
  # xxx parameter
  parm=list()
  parm$scale='Patient'
  parm$a=0.5
  npt=length(unique(Id))
  parm$predict.model=model.name
  parm$pval_threshold=1000
  # Opening backend-configuration
  cl <- makeSOCKcluster(detectCores())
  registerDoSNOW(cl)
  # Getting models and predictions
  prdC=foreach(i=seq(npt),.packages = c("dplyr", "tibble", "matrixStats"), .export = c("CYTOF")) %dopar% xxx(data.matrix(restricted.CYTOF), Yh, Id, i, parm, TRUE)
  list_preds<-aggregation.of.predictions(prdC)
  # Closing backend configuration
  stopCluster(cl)
  # Saving the models and their predictions
  save(prdC, file=paste0(out_path, "/", model.name," on best features.rda"))
  save(list_preds, file=paste0(out_path, "/", model.name," on best features predictions.rda"))
}
```

Plotting the new models

```{r}
# X axis
x_axis="DOS" # EGA or DOS

# Plotting the results of the models on best features
for (model.name in names(best_pval_thresholds)){
  load(paste0(out_path, "/", model.name," on best features predictions.rda"))
  y_pred <- list_preds[[model.name]]
  pdf(file=paste0(plot_path, "/", model.name, " on best features on ",x_axis," outcomes.pdf"))
  plot.model.results(y_pred[preterm_rows], Yh[preterm_rows], EGA[preterm_rows], paste0(model.name, " on preterm"), x_axis)
  plot.model.results(y_pred[-preterm_rows], Yh[-preterm_rows], EGA[-preterm_rows], paste0(model.name, " on term"), x_axis)
  plot.model.results(y_pred, Yh, EGA, paste0(model.name, " on all"), x_axis)
  dev.off()
}
```

Looking at the number of selected features

```{r}
for (model.name in names(best_pval_thresholds)){
  feature.index <- get.feature.index(model.name, on.best.feature = TRUE, CYTOF.data = restricted.CYTOF)
  selected.features <- feature.index[feature.index[,"model_index"]>0, "feature"]
  print(paste0(model.name, " : ", length(selected.features), " selected features"))
}
```
Choosing one model to calculate the scores

```{r}
model.name = "Random.Forest" # Lasso.1se or Ridge.Regression or ElasticNet or Adaptive.Lasso or Random.Forest
```

Saving feature.index

```{r}
feature.index <- get.feature.index(model.name, on.best.feature = TRUE, CYTOF.data = restricted.CYTOF)
feature.index
save(feature.index, file=paste0(out_path, "/feature_index.rda"))
```

## Overlap with OOL paper

```{r}
# List of features
x <- list(OOL_paper = c("CD56loCD16posNK_STAT1_IFNa",
                        #"Granulocytes",
                        "CD4Tnaive_MAPKAPK2_IFNa",
                        "ncMCs_CREB_GMCSF",
                        "CD8Tem_MAPKAPK2_unstim",
                        "pDCs_STAT1_IFNa",
                        "Bcells_MAPKAPK2_LPS",
                        "CD4Tem_MAPKAPK2_unstim",
                        "CD8Tem_MAPKAPK2_IFNa",
                        #"Bcells",
                        "CD4Tem_NFkB_IL246",
                        "CD4Tcm_IkB_unstim",
                        #"mDCs_STAT6_IFNa",
                        "pDCs_STAT6_IFNa",
                        #"mDCs_MAPKAPK2_unstim", 
                        "pDCs_MAPKAPK2_unstim"), 
          Top_model_index = feature.index %>%
                                dplyr::filter(model_index > 0.) %>%
                                dplyr::select(feature) %>%
                                unlist() %>%
                                as.character())

ggVennDiagram(x, label_alpha = 0)
```
## Importing selected model and its predictions

Importing the model

```{r}
# We are interested in the model built on best features
on.best.feature = TRUE

if (on.best.feature){
    # list_preds
    load(paste0(out_path, "/", model.name," on best features predictions.rda"))
    # prdC
    load(paste0(out_path, "/", model.name," on best features.rda"))
  }else{
    # list_preds
    load(paste0(out_path, "/best ", model.name," predictions.rda"))
    # prdC
    load(paste0(out_path, "/best ", model.name,".rda"))
  }

# key to fetch the model's coefs in prdC
if (model.name == "Random.Forest"){
  coef_key <- "rd_forest_importance"
}else if(model.name == "ElasticNet"){
  coef_key <- "EN_coef"
}else if(model.name == "Lasso.1se"){
  coef_key <- "coef"
}else if(model.name == "Ridge.Regression"){
  coef_key <- "ridge_coef"
}else if(model.name == "Adaptive.Lasso"){
  coef_key <- "alasso_coef"
}else{
  coef_key <- NULL
}
```

## Patients clustering

Here we want to cluster the patients of the OOL data the 4 patients from the drug assay study.

```{r}
# Population_reagent_stim of interest
# population_reagent_stim_interest <- subset(feature.index, model_index > 0, select=feature) %>% 
#                                         unlist()
# Population_reagent_unstim of interest
population_reagent_unstim_interest <- subset(feature.index, model_index > 0, select=feature) %>%
                                        dplyr::mutate(feature=paste0(str_extract(feature, ".*(?=_[^_]*$)"),"_unstim")) %>%
                                        distinct(feature) %>%
                                        unlist()
# Setting the time to labor of the four patients from the drug assay study
centroids.TTL <- c(-79,-81,-81,-70)
# Getting the centroids (4 patients)
drug_assay_unstim_0 <- subset(read.csv(paste(drug_assay_path, "/Preprocessed Data/preprocessed_pen.csv", sep = "/")),
                                             dose==-1,
                                             select= -c(X, dose)) %>%
                                       dplyr::mutate(feature.name=paste(population, reagent, stimulation, sep="_")) %>%
                                       dplyr::filter(feature.name %in% population_reagent_unstim_interest)

# drug_assay_stim_0 <- subset(read.csv(paste(drug_assay_path, "/Preprocessed Data/preprocessed_pen.csv", sep = "/")),
#                                              (dose %in% c(-1,0) & !(stimulation=="unstim" & dose==0) ), #
#                                              select= -c(X,dose)) %>%
#                                      dplyr::mutate(feature.name=paste(population, reagent, stimulation, sep="_")) %>%
#                                      dplyr::filter(feature.name %in% population_reagent_stim_interest)

centroids <- drug_assay_unstim_0 %>%
                    group_by(feature.name, ID) %>%
                    dplyr::summarise(feature=median(feature)) %>%
                    pivot_wider(names_from = ID, values_from = feature) %>%
                    column_to_rownames("feature.name")

# Joining the TTL prediction and the Id of the patients to the CYTOF data
OOL_data <- read_csv(paste0(OOL_path, "/Preprocessed Data/immunome_EGA_pen_OOL.csv"), show_col_types = FALSE)
OOL_unstim <- OOL_data[, (grepl("unstim", colnames(OOL_data)) & (colnames(OOL_data) %in% rownames(centroids)))] %>%
                cbind(data.frame("TTL"=list_preds[[model.name]], "ID"=Id))
# OOL_stim <- OOL_data[, colnames(OOL_data) %in% rownames(centroids)] %>%
#                 cbind(data.frame("TTL"=list_preds[[model.name]], "ID"=Id))

# Normalization of the data
for (target_feature in rownames(centroids)){
  # Mean and std from drug assay data
  mu_DA <- subset(drug_assay_unstim_0, feature.name==target_feature, select=feature) %>% unlist() %>% mean(na.rm=T)
  sd_DA <- subset(drug_assay_unstim_0, feature.name==target_feature, select=feature) %>% unlist() %>% sd(na.rm=T)
  # Mean and std from OOL data
  mu_OOL <- subset(OOL_data, select=target_feature) %>% unlist() %>% mean(na.rm=T)
  sd_OOL <- subset(OOL_data, select=target_feature) %>% unlist() %>% sd(na.rm=T)
  # Rescaling the features
  centroids[target_feature,] <- (centroids[target_feature,] - mu_DA)/sd_DA
  OOL_unstim[,target_feature] <- (OOL_unstim[,target_feature] - mu_OOL)/sd_OOL
}
# Adding the TTL predictions to the centroids
centroids["TTL",] <- centroids.TTL

# Clusters initialization
clusters <- list(c(), c(), c(), c())
data_points <- c()
# Filling the cluster patient per patient
for (id in unique(Id)){
  # Getting all the samples from the same patient
  samples <- subset(OOL_unstim, ID==id, select=-ID) %>% remove_rownames() %>% t()
  colnames(samples) <- seq(colnames(samples))

  # Calculating the euclidian distances between the patient and the centroids
  list_dist <- list()
  for (centroid_id in seq(centroids.TTL)){
    centroid_data <- centroids %>% dplyr::select(centroid_id) %>% setNames("centroid")
    dist_euclid <- cbind(centroid_data, samples) %>%
                      # Calculating (sample - centroid)^2
                      dplyr::mutate(across(where(is.numeric), ~ (. - centroid)^2)) %>%
                      # Selecting sample with closest TTL to the centroid TTL
                      dplyr::select(-centroid) %>%
                      t() %>%
                      as.data.frame() %>%
                      dplyr::filter(TTL==min(TTL)) %>%
                      # Calculating euclidian distance with centroid
                      dplyr::select(-TTL)%>%
                      unlist() %>%
                      sum(na.rm=T) %>%
                      sqrt()
    list_dist[[centroid_id]] <- dist_euclid
  }
  # Assigning the id to one of the four clusters
  closest_centroid <- which.min(unlist(list_dist))
  clusters[[closest_centroid]] <- c(clusters[[closest_centroid]], id)
  # Getting the sample used for the clustering
  sample <- samples %>% t() %>% as.data.frame() %>% 
              dplyr::mutate(dist.TTL=abs(TTL-centroids.TTL[closest_centroid]),
                            cluster=closest_centroid) %>% 
              subset(dist.TTL==min(dist.TTL), select=-c(TTL, dist.TTL)) 
  data_points <- rbind(data_points, sample)
}

# Creating new directory
if (!file.exists(paste0(OOL_path, "/Patients clustering"))){dir.create(paste0(OOL_path, "/Patients clustering"), recursive = TRUE)}

# Saving the clusters
save(clusters, file=paste0(OOL_path, "/Patients clustering/Cluster 53 OOL patients around 4 drug assay patients.rda"))

# Adding the centroids to the data_points used for the clustering
colnames(centroids) = c("centroid1","centroid2","centroid3","centroid4")
data_points <- rbind(data_points,
                     centroids[!(rownames(centroids) %in% c("TTL")),] %>% t() %>% cbind(matrix(c(1, 2, 3, 4), ncol=1, nrow=4, dimnames=list(NULL, "cluster"))))
```

Plotting the clusters (you can look at the plot in the folder /Plots/Clustering)

```{r}
# Filter out variables with variance not equal to zero or with too many NA
filtered_data <- data_points %>% 
                  select_if((~sd(., na.rm = TRUE) != 0)) %>%
                  select_if(~ !any(is.na(.)))

# Separating cluster number from actual coordinates
patient_clusters  <- data_points$cluster %>% as.character()
data_pca <- dplyr::select(filtered_data, -cluster)

# Determining the optimal number of principal components to represent our data
nb <- estim_ncpPCA(data_pca, method.cv = "Kfold", verbose = FALSE)

# Using PCA to project the points on a 2D surface
imp <- cbind.data.frame(data_pca, patient_clusters)
res.pca <- PCA(imp, quanti.sup = 1, quali.sup = dim(imp)[2], ncp = nb$ncp, graph=FALSE)

# Dataframes for plot
projected_coord <- cbind(res.pca$ind$coord[,c("Dim.1", "Dim.2")], patient_clusters) 
df_coord <- as.data.frame(projected_coord) %>% 
              rownames_to_column("ID") %>%
              dplyr::filter(!grepl("centroid", ID)) %>% 
              dplyr::mutate(Dim.1=as.double(Dim.1), Dim.2=as.double(Dim.2))
df_centroid <- as.data.frame(projected_coord) %>% 
              rownames_to_column("ID") %>% 
              dplyr::filter(grepl("centroid", ID))%>% 
              dplyr::mutate(Dim.1=as.double(Dim.1), Dim.2=as.double(Dim.2))

# Plot with all the points
p1 <- ggplot() +
  geom_point(data=df_centroid, aes(x=Dim.1, y=Dim.2, color=patient_clusters, shape="Drug assay centroids"), size=3) +
  geom_point(data=df_coord, aes(x=Dim.1, y=Dim.2, color=patient_clusters, shape="OOL patients")) +
  scale_shape_manual(name = "Patients", values = c("OOL patients" = 21, "Drug assay centroids" = 8)) +
  guides(shape = guide_legend(title = "Patients", order=2), color=guide_legend(title = "Clusters", order=1)) +
  scale_x_continuous(limits=c(-5, 7.5), expand=c(0., 0.)) +
  scale_y_continuous(limits=c(-5, 5), expand=c(0., 0.)) +
  ggtitle("Projection of patient clustering on 2 first dimensions of PCA analysis") +
  theme_minimal()

# Plot with only the centroids
p2 <- ggplot() +
  geom_point(data=df_centroid, aes(x=Dim.1, y=Dim.2, color=patient_clusters, shape="Drug assay centroids"), size=3) +
  scale_shape_manual(name = "Patients", values = c("OOL patients" = 21, "Drug assay centroids" = 8)) +
  guides(shape = guide_legend(title = "Patients", order=2), color=guide_legend(title = "Clusters", order=1)) +
  scale_x_continuous(limits=c(-5, 7.5), expand=c(0., 0.)) +
  scale_y_continuous(limits=c(-5, 5), expand=c(0., 0.)) +
  ggtitle("Projection of centroids on 2 first dimensions of PCA analysis") +
  theme_minimal()

# Creating new directory
if (!file.exists(paste0(my_path, "/Plots/Clustering"))){dir.create(paste0(my_path, "/Plots/Clustering"), recursive = TRUE)}

# Saving the clustering plots
pdf(paste0(my_path, "/Plots/Clustering/Projection of patient clustering with PCA dim1&2.pdf"), height=5, width=7)
p1
p2
dev.off()
```

## Visualization of simulated drug effect 

All simulated predictions

```{r}
all.simulated.predictions <- c()
col_names <- c("drug", "sampleID", "centroid", "dose", "TTL.prediction")
for (i in 1:length(prdC)){
  all.simulated.predictions <- rbind(all.simulated.predictions, 
                                     prdC[[i]]$median.simulated.TTL %>% rename_all(~col_names),
                                     prdC[[i]]$individual.simulated.TTL %>% 
                                        dplyr::mutate(patientID = str_extract(sampleID, "\\d+")) %>% 
                                        rowwise() %>% dplyr::filter(patientID %in% clusters[[centroid]]) %>% 
                                        dplyr::select(-patientID) %>% 
                                        rename_all(~col_names))
}
```

Function plotting the distributions of the differences between estimated TTL and estimated TTL with simulated effect of the 15 drugs at concentration c_k (with c_k in {1, 2, 3, 4}) clustered around centroid.val (with centroid.val in {1, 2, 3, 4, median}).

```{r}
plot_delay_distribution <- function(c_k, centroid.val="median", EGA.inf=0, EGA.sup=40){
  # Filtering the samples corresponding to EGA between EGA.inf and EGA.sup days
  filter.ID <- subset(read_csv(paste0(OOL_path, "/Preprocessed Data/EGA_OOL.csv"), show_col_types = FALSE),
                      EGA >= EGA.inf & EGA <= EGA.sup,
                      select = ID)$ID
  
  # Getting the simulated predictions
  drug_dose_simulated_predictions <- dplyr::filter(all.simulated.predictions, (centroid==centroid.val)&(dose==10^(c_k-1))) %>%
                                      pivot_wider(names_from = drug, values_from = TTL.prediction) %>% column_to_rownames("sampleID") %>%
                                      dplyr::select(-c(centroid, dose))
  
  # Getting the row numbers of the predictions corresponding to simulated.data
  row_numbers <- which(data$ID %in% filter.ID)
  
  # TTL predictions
  TTL.predictions <- matrix(list_preds[[model.name]][row_numbers], 
                            nrow = length(row_numbers), 
                            ncol = dim(drug_dose_simulated_predictions)[2], 
                            dimnames = list(filter.ID, colnames(drug_dose_simulated_predictions)))
  
  # Difference between the TTL predictions and the TTL predictions with simulated drug effect
  drug.effect.on.predictions <- - (drug_dose_simulated_predictions[filter.ID,] - TTL.predictions[filter.ID,])

  # Looking at the distributions of the drug simulated effect in term of days on the TTL prediction
  plot.data <- as.data.frame(drug.effect.on.predictions)
  
  # Reshape data into long format
  plot.data_long <- gather(plot.data, key = "drug", value = "value")

  # Plot distributions
  p <- ggplot(plot.data_long, aes(x = value, fill = drug)) +
    geom_density(alpha = 0.5) +
    xlab("Delay (in days)") +
    geom_vline(xintercept = 0, linetype = "dashed") +
    ggtitle(paste0("Dose ", 10^(c_k-1))) +
    theme_minimal()
  plot(p)
}

# Creating a new directory
if (!file.exists(paste0(my_path, "/Plots/Drug effect delay distributions"))){dir.create(paste0(my_path, "/Plots/Drug effect delay distributions"))}

# Plotting all the combinations of plots possible
for (centroid.val in unique(all.simulated.predictions$centroid)){
  pdf(paste0(my_path, "/Plots/Drug effect delay distributions/Delay distribution for ", ifelse(centroid.val=="median","median effect", paste("cluster",centroid.val)), ".pdf"))
  for (c_k in 1:4){
    plot_delay_distribution(c_k, centroid.val)
  }
  dev.off()
}
```

Function plotting the distributions of the differences between estimated TTL and estimated TTL with simulated effect for a given drug and its concentration c_k (with c_k in {1, 2, 3, 4}) across the four clusters.

```{r}
plot_delay_distribution_across_clusters <- function(target_drug, c_k, EGA.inf=0, EGA.sup=40){
  # Filtering the samples corresponding to EGA between EGA.inf and EGA.sup days
  filter.ID <- subset(read_csv(paste0(OOL_path, "/Preprocessed Data/EGA_OOL.csv"), show_col_types = FALSE),
                      EGA >= EGA.inf & EGA <= EGA.sup,
                      select = ID)$ID
  
  # Getting the simulated predictions
  drug_dose_simulated_predictions <- dplyr::filter(all.simulated.predictions, (drug==target_drug)&(dose==10^(c_k-1))) %>%
                                      pivot_wider(names_from = centroid, values_from = TTL.prediction) %>% column_to_rownames("sampleID") %>%
                                      dplyr::select(-c(drug, dose, median))
  
  # Getting the row numbers of the predictions corresponding to simulated.data
  row_numbers <- which(data$ID %in% filter.ID)
  
  # TTL predictions
  TTL.predictions <- matrix(list_preds[[model.name]][row_numbers], 
                            nrow = length(row_numbers), 
                            ncol = dim(drug_dose_simulated_predictions)[2], 
                            dimnames = list(filter.ID, colnames(drug_dose_simulated_predictions)))
  
  # Difference between the TTL predictions and the TTL predictions with simulated drug effect
  drug.effect.on.predictions <- - (drug_dose_simulated_predictions[filter.ID,] - TTL.predictions[filter.ID,])

  # Looking at the distributions of the drug simulated effect in term of days on the TTL prediction
  plot.data <- as.data.frame(drug.effect.on.predictions)
  
  # Reshape data into long format
  plot.data_long <- gather(plot.data, key = "centroid", value = "value")

  # Plot distributions
  p <- ggplot(plot.data_long, aes(x = value, fill = centroid)) +
    geom_density(alpha = 0.5) +
    xlab("Delay (in days)") +
    geom_vline(xintercept = 0, linetype = "dashed") +
    ggtitle(paste0("Dose ", 10^(c_k-1))) +
    theme_minimal()
  plot(p)
}

# Creating a new directory
if (!file.exists(paste0(my_path, "/Plots/Drug effect delay distributions"))){dir.create(paste0(my_path, "/Plots/Drug effect delay distributions"))}

# Plotting the best drug concentration effect on the different clusters
target_drug = "Progesterone"
c_k = 4

pdf(paste0(my_path, "/Plots/Drug effect delay distributions/Delay distribution for ", target_drug, " dose=", 10^(c_k-1), " across 4 clusters.pdf"))
plot_delay_distribution_across_clusters(target_drug, c_k)
dev.off()
```
